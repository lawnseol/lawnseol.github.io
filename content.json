{"pages":[],"posts":[{"title":"Artificial Intelligence","text":"인공지능 연구의 목적 인공지능을 다루는 과학자의 대부분은 인공지능을 통해서 인간의 지능이나 뇌의 기능을 해명하고 싶다. 인공지능을 통해서 인간의 지능이나 기능을 구현하고 싶다. 인공지능 개념적 목표 인간이 가진 지능을 만들고 싶다 인간과 동일한 구조를 가지고 인간처럼 동작하는, 행동 뿐만 아니라 내부 구조까지 인간과 같은 것을 만들고 싶다. 인간의 지능으로 실현할 수 있는 기능을 만들고 싶다 구조가 완전하게 인간과 같은지는 제쳐두고 표면적인 행동이나 기능을 구현하는 방법만을 생각하는 것으로 인간과 완벽하게 같은 지능을 만들기 어렵기 때문에 현재 인공지능을 연구하는 목표이다. 인간의 지능으로도 실현하기 어려운 기능을 만들고 싶다 인간이 가진 지능에만 국한되지 않고 그보다도 더 굉장한 지능을 목표로 한다. 인공지능에서의 지식 인공지능의 역사는 인공지능을 똑똑하게 하기 위해 지식을 가르치는 구조를 해석하고 정의하면서 발전했다고 볼 수 있다. 인공지능의 역사를 이해하기 위해서 우선 지식이란 무엇인지 알아보기로 한다. 사전적 정의 어떤 대상에 대하여 배우거나 실천을 통하여 알게 된 명확한 인식이나 이해. 알고 있는 내용이나 사물. 철학 인식에 의하여 얻어진 성과. 사물에 대한 단편적인 사실적ㆍ경험적 인식을 말하며, 객관적 타당성을 요구할 수 있는 판단의 체계를 이른다. 사전적 의미로 보자면 지식이란 곧 어떤 사람이 ‘알고 있는(인식)’, 또는 ‘(이해)하고 있는’ 것을 말한다. 지식 습득 조건 어떤 인물 A가 어떠한 사항 P를 알고 있다를 정의하려면 다음 세가지 조건을 만족해야 한다. A는 P를 신뢰하고 있다. P는 사실이다. A는 P를 신뢰할 만한 이유가 있다. 만약 A가 '지금은 10시다’라는 사항을 알고 있다고 가정하고, 조건 1만 만족했다면 지금 상황은 A가 '지금은 10시다’라고 믿고 있을 뿐입니다. 그저 그 사람이 그렇게 생각하고 있을 뿐인데 그 사람이 '알고 있다’고 하기엔 무리가 있다. 조건 2를 더한다고 하면 A가 '지금 10시다’라고 믿고 있고, 실제로도 시간이 10시가 된다. 이 경우는 사실 A가 아무 근거없이 그저 '지금은 10시다’라고 생각했을 뿐인데 우연히 현재 10시였을 가능성도 있기 때문이다. 조건 3을 더하면 A가 눈앞의 시계를 보고 있고 시계가 10시를 가리키고 있는 경우 A가 '지금 10시다’라는 사실을 알고 있다고 할 수 있다. 하지만 여기서 시계가 고장났다면, 오전10시인지 오후 10시인지 등 다양한 부분에 대해 지식의 종류 지식표현방법 역사 제1차 부흥기 인공지능은 1940년대부터 연구하기 시작했지만 인공지능(AI, Artificial Intelligence)라는 표현은 1956년 '다트머스 회의’가 최초였다. 다트머스 회의는 인공지능에 관한 연구 발표의 장으로 당시 컴퓨터상에서 기호 처리를 통해 퍼즐 등을 고속으로 풀 수 있다는 것이 판명되었고, 연구가 진행됨에 따라 모든 사람이 컴퓨터의 가능성을 높이 사고 있는 시대였다. 이때 주된 개념은 '지능이란 곧 기호 처리’였다. 대화시스템에 대한 연구도 시작되었는데 ELIZA(일라이자)라는 대화시스템은 상담 치료사 역할을 하면서 텍스트 채팅을 통해 인간과 대화를 하는 방식이었다. ELIZA는 자신이 먼저 능동적으로 무언가를 말하지는 않고 사용자의 이야기를 수동적으로 받아들일 뿐인 시스템으로 이런 대화를 반복하면서 사용자가 점점 '나에 대해 이해해준다’는 느낌을 받게 구성되어 있었다. SHRDLU라는 대화시스템은 쌓기 블록이 표시된 블록을 옮기는 시스템으로 블록을 움직이기 위한 명령이나 순서를 처리할 수 있도록 구성되었다. 인공지능을 퍼즐과 같은 문제(Toy problem)는 풀 수 있어도 사회에 실제로 도움이 될 법한 문제에는 손 쓸 방법이 없었기 때문에 제1차 부흥기는 금방 끝나고 만다. 이러한 문제들을 '프레임 문제’라고 불렀다. 프레임 문제는 인공지능이 문제를 해결할 때 고려해야 할 범위(프레임)를 제대로 설정하지 못하는 문제를 말한다. 예를 들어 로봇이 짐을 가지러 방에 들어간다고 할 때 로봇은 '짐’이라는 목적에만 초점을 맞추고 있기 때문에 짐 위에 꽃병이 높여있더라도 전혀 신경 쓰지 않고 짐을 들어 올려서 꽃병을 깨고 말 것이다. 그렇다고 짐 주변을 모두 신경쓰게 되면 로봇이 고려할 범위가 끝도 없이 늘어나게 된다. 이처럼 컴퓨터가 적절한 범위를 제대로 정할 수 없는 문제를 가리켜 '프레임 문제’라고 한다. 제2차 부흥기 전문가 시스템(Expert System)으로 전문가의 지식을 컴퓨터에 집어넣은 뒤, 그 지식에 기초하여 판단을 내리는 방식으로 제2차 부흥기가 시작되었다. 예를 들어 의사가 환자를 진찰할 때에 여러가지 증상을 관찰하고 이를 기반으로 어떠한 치료와 투약을 하면 좋을지 판단한다. 이 때 의사가 사용하는 지식을 컴퓨터에 입력해둠으로써 컴퓨터가 마치 의사처럼 판단을 내리게 한다. 이 때 주된 개념은 '지능이란 곧 지식이다’였다. 이전까지의 연구로 컴퓨터가 스스로 현실 세계에 대해 고려하기는 힘들다는 사실이 판명되었으므로 인간이 가르쳐서 똑똑하게 만들어주자는 생각을 했었고 대화시스템에서도 지식을 갖춤으로써 고도의 대화를 할 수 있다는 것이 확인되었다. 대표적으로 'GUS(genial understander system)'를 들 수 있다. 하지만 말로 설명할 수 있는 지식은 컴퓨터에 입력할 수 있지만 전문가라면 말로 설명하기 힘든 지식도 가지고 있게 되는데 컴퓨터에게는 ‘대체로’, '적당히’와 같은 정량화되지 않은 부분은 입력이 어려웠다. 이러한 문제를 지식 획득 병목(Knowledge acquisition bottleneck)라고 불렀고 이러한 지식을 얻기 위한 비용이 너무 높아지게 되어서 제2차 부흥기도 금방 사그라들게 된다. 제3차 부흥기 2010년 이후부터 딥러닝으로 인해 비약적인 발전을 이루게 되는데 이미지 분야 같은 경우는 인공지능이 인간을 능가하게 되었다. 딥리닝의 특징은 인간이 개입하지 않아도 특징을 자동으로 찾아낸다는 데 있다. 이전 전문가 시스템까지는 인간이 특정 조건과 지식을 입력해주면 컴퓨터가 처리하는 식으로 진행되었으나, 딥러닝은 이런한 과정을 자동으로 처리하게 개발되었다. 이러한 현상을 바탕으로 주된 개념은 '지능이란 곧 학습이다’였다. 딥러닝에서는 대량의 데이터를 기반으로 학습하여 규칙성을 발견함으로써 대상을 분류하게 된다. 예를 들어 Google의 고양이(Using large-scale brain simulations for machine learning and A.I.)를 분류하기 위해 레이블을 달아놓은 학습데이터 25,000장, 테스트데이터 12,500장으로 구성하여 학습을 하고 이를 활용하여 분류를 했다. 딥러닝을 활용한 연구 개발이 매우 활발해지면서 레이블이 달린 혹은 분류되어 있는 대량의 데이터가 필요하게 되었고 데이터를 모으는 일 자체가 큰 비용이 들어가는 아이러니한 현상이 벌어지게 되었다. 평가방법 튜링테스트 튜링테스트는 인공지능이 인간과 같은 반응을 할 수 있는지를 판단하기 위한 테스트로 1950년 '이미테이션 게임’이라는 이름으로 튜닝이 발안하였다. 예를 들어 어떤 사람이 다른 곳에 있는 두 사람과 채팅으로 이야기를 하고 있는데 이야기 대상인 두 사람 중 하나는 인간, 하나는 컴퓨터이다. 그리고 테스터가 둘 중 어느 쪽이 인간이고 어느 쪽이 컴퓨터인지를 판별하지 못하면 그 컴퓨터는 튜링 테스트에 합격하게 된다. 튜링은 발안할 당시에 '5분 동안 이야기를 나눠본 뒤 30%의 사람이 속는다면 그 시스템은 튜링 테스트에 통과했다고 볼 수 있다.'고 합격기준을 제시하였다. '유진(Eugene Goostman)'이라는 13세 우크라이나인 설정을 가진 챗봇은 튜닝테스트를 통과했다고 발표한 적이 있다. 그러나 테스트 시간이 너무 짧고 우크라이나인이라는 설정 때문에 영어 문법이나 이야기하는 내용이 다소 이상해도 넘어가기 쉬웠다는 점에서 튜닝테스트를 완벽히 통과했다고 보기는 어렵다. 또한 튜링테스트를 완벽히 통과했다고 해서 인공진능이 완벽해지는 것이 아니고 ‘유진’ 사례에서 보는 바와 같이 결과 뿐만 아니라 환경, 과정까지 모두 살펴보아야 한다. 중국어 방 튜링테스트에 대한 대표적인 반론으로는 중국어 방(Chinese Room)을 들 수 있다. 이것은 내부 구조와 관계없이 표면적인 행동에만 주목하여 평가하는 '기능주의’에 대한 회의적인 입장에 기반한 사고 실험에서 출발하여 존 설(John Searle)이 발표하였다. 중국어 방의 사고실험을 통해 '튜링 테스트는 대화가 성립하는 것만으로 통과라 보지만, 그것만으로는 정말 인간과 같이 내용을 이해했다고 할 수 없다’고 반론하였고 강한 인공지능(Strong AI, General AI)과 약한 인공지능(Weak AI, Narrow AI)를 말하게 되었다. 강한 인공지능 : 정말 스스로 생각하는 듯한 인공지능 약한 인공지능 : 스스로 생각하지 않고 정해진 절차에 따라 처리를 수행할 뿐인 인공지능 각 분야별 평가방법 분류 정밀도 예를 들어 100장의 이미지가 있고 각각 고양이, 개, 인간 라벨이 있는 경우 각 이미지가 어느 이미지인지를 인공지능에게 판별시킨 뒤 정답률을 산출시킨다. $$ 분류 정밀도 = \\frac{올바르게 분류한 수}{대상 총 수} $$ 오인식률 음성인식의 경우 얼마만큼의 단어를 잘못 인식했는가를 나타내는 비율(Word Error Rate)로 계산한다. 음식인식의 오류에는 대치오류[1], 삭제오류[2], 삽입오류[3]가 있으며 이 오류들의 수를 원래 인식했어야 할 단어의 수로 나누면 오인식률이 된다. $$ 오인식률 = \\frac{오류수의 총합}{인식해야 하는 총수} $$ 주관평가 기계 번역이나 대화 시스템과 같은 분야에서는 정답을 하나로 단정하기 어렵다. 이러한 분야에서는 많은 유저에게 실제로 기술을 사용하게 한 뒤, 설문조사를 통해 평가를 진행하는 방식(주관평가, Subjective Evaluation)을 채택한다. 단, 평가가 주관적으로 이루어지므로 보다 많은 사람에게 평가를 받아서 결과를 평균화시켜야 하며, 이 때문에 매번 많은 사람을 동원하다간 비용이 높아지고 만다. 그래서 대부분 '자동평가척도’를 사용하는 경우가 많다. 자동 평가 측도 기계번역을 예로 들면, 많은 수의 문장을 사람과 시스템에게 각각 번역시킨다. 그 다음에는 사람의 번역과 기계의 번역을 비교해서 번역을 평가하기 위한 평가 척도를 정한다. 예를 들어 사람의 번역에 들어있는 표현이 기계의 번역에도 들어있다면 좋은 번역일 가능성이 높으므로 '사람의 번역에 들어있는 표현의 개수’가 평가 척도가 된다. 그 평가 척도의 타당성을 '상관 계수’를 사용하여 검증한다. 참고로 현재 기계 번역에서 사용되는 평가 척도 중 가장 유명한 것은 'BLUE(Bilingual Evaluation Understudy)'이다. 일단 자동 평가 척도가 결정되고 나면 대역 데이터에 대해 평가가 높아지도록 알고리즘 연구 개발을 진행한다. 그러면 평가에 대한 사람의 개입이 줄어들게 되어 개발 비용이 현저히 낮아질 수 있다. 기초 난수발생 신경망을 쉽게 정의해보면 많은 숫자로 구성된 행렬이라고 할 수 있다. 이 행렬에 어떤 입력을 넣으면 출력을 얻게 되는데 행렬을 구성하는 숫자는 처음에는 랜덤한 값을 지정해줄 수 밖에 없다. 처음에는 랜덤한 값을 지정해줄 수 밖에 없다. 처음에 신경망의 초기값을 지정해주는 것을 초기화(Initialization)라고 하며 Xavier초기화와 He초기화가 많이 쓰인다. 이 방법들은 랜덤하지만 어느정도 규칙성이 있는 범위 내에서 난수를 지정한다. tensorflow에서는 tf.random.uniform 함수를 불러오면 균일 분포(uniform distribution)의 난수를 얻을 수 있다. 균일 분포는 최솟값과 최댓값 사이의 모든 수가 나올 확률이 동일한 분포에서 수를 뽑는다는 뜻이다. 1Tensorflow Code tf.random.normal 함수를 불러오면 정규 분포(normal distribution)의 난수를 얻을 수 있다. 정규 분포는 가운데가 높고 양극단으로 갈수록 낮아져서 종 모양을 그리는 분포를 말한다. title1234import tensorflow as tfrand = tf.random.uniform([1],0,1)print(rand) 배열곱 리스트에 정수를 곱하면 양수일 경우 숫자만큼 리스트의 원소를 반복하고, 0 이하일 경우 빈 리스트를 반환하고 실수를 곱하면 에러를 발생한다. 1234import tensorflow as tfrand = tf.random.normal([4],0,1)print(rand) numpy를 사용해 array에 숫자를 곱하게 되면 각 원소에 대해 자동으로 숫자를 곱하는 연산이 이루어지는데, 이를 각 원소에 대한 연산(element-wise operation)이라고 한다. 12345import numpy as npprint(np.array([1,2,3])*2)print(np.array([1,2,3])*0)print(np.array([1,2,3])*-1)print(np.array([1,2,3])*0.01) 뉴럽(퍼셉트론, 신경세포) 신경망은 뉴런이 여러 개 모여 레이어(층)를 구성한 후 이 레이어가 다시 모여 구성한 형태이다. 뉴런은 입력, 가중치, 활성화함수, 출력으로 구성된다. $$ Y=f(X \\times w)\\ \\ \\scriptsize 입력치 X,가중치 w, 출력 Y, 활성화함수 f $$ 뉴런에서 학습할 때 변하는 가중치는 처음에는 초기화를 통해 랜덤한 값을 넣고, 학습 과정에서 점차 일정한 값으로 수렴한다. 가중치 조정을 통해 원하는 출력에 가깝게 조정하여 모델을 향상시키게 된다. 결국 간단하게 보면 \\(w\\)가 뉴런이라 생각할 수 있다. 활성화 함수로는 시그모이드(sigmoid), ReLU(Rectified Linear Unit) 등을 많이 사용하게 된다. 신경망 초창기에는 시그모이드가 주로 쓰였지만 은닉층을 많이 사용하는 딥러닝에서 오류를 역전파(Backpropagate)할 때 시그모이드 함수가 값을 점점 작아지게 하는 문제를 2010년 논문[4]토론토 대학교의 비노드 네어와 제프리 힌튼 교수가 지적하면서 ReLU를 대안으로 제시하였다. 시그모이드 함수는 출력값을 0~1 사이로만 제한되어 있지만 ReLU는 양수를 그대로 반환하기 때문에 값의 왜곡이 적어진다. 예를 들어 시그모이드 함수를 사용하여 1을 입력할 경우 기대출력이 0이 되는 뉴런을 만들어보면 아래와 같다. 1234567891011121314151617import mathimport tensorflow as tfdef sigmoid(x): return 1 / (1 + math.exp(-x))x = 1y = 0w = tf.random.normal([1],0,1)for i in range(1000): output = sigmoid(x * w) error = y - output w = w + x * 0.1 * error if i%100 == 99: print(i, error, output) 이 코드에서는 경사하강법(Gradient Descent)를 사용했는데 \\(w\\)에 입력\\(x\\), 학습률\\(\\alpha\\), 에러\\(error\\)의 곱을 합한 값으로 학습률이 너무 큰 경우 학습이 빠르지만 적정한 수치를 벗어날 우려가 있고, 너무 작은 값은 학습 속도가 너무 느려질 수 있다. 1234567891099 -0.08803679043528755 0.08803679043528755199 -0.048206592326328714 0.048206592326328714299 -0.032931402098842766 0.032931402098842766399 -0.024941436740358217 0.024941436740358217499 -0.020047549096087545 0.020047549096087545599 -0.01674840932694568 0.01674840932694568699 -0.014376145500721372 0.014376145500721372799 -0.012589411771020347 0.012589411771020347899 -0.011195832981499537 0.011195832981499537999 -0.010078814583572272 0.010078814583572272 위 결과에 보면 error는 0에 점점 가까워지고 output도 기대출력인 0에 가까워지는 것을 확인할 수 있다. 그런데 입력값과 기대출력을 바꾸면 동일한 결과가 나올까? 간단히 수식에서 생각해보면 \\(x\\) 가 0이기 때문에 값이 변하지 않게 된다. 이런 경우를 방지하기 위해 편향(bias)를 추가하게 되며 아래와 같이 사용하게 된다. $$ Y=f(X \\times w + 1 \\times b) \\ \\ \\scriptsize 입력치 X,가중치 w, 출력 Y, 편향 b, 활성화함수 f $$ 1234567891011121314151617181920import mathimport tensorflow as tfdef sigmoid(x): return 1 / (1 + math.exp(-x))x = 0y = 1alpha = 0.1w = tf.random.normal([1],0,1)b = tf.random.normal([1],0,1)for i in range(1000): output = sigmoid(x * w + 1* b) error = y - output w = w + x * alpha * error b = b + 1 * alpha * error if i%100 == 99: print(i, error, output) 1234567891099 0.07355049116562629 0.9264495088343737199 0.04336122942384402 0.956638770576156299 0.03056554939033551 0.9694344506096645399 0.0235506250070594 0.9764493749929406499 0.019135113381853364 0.9808648866181466599 0.016104868965294172 0.9838951310347058699 0.013898372317313057 0.9861016276826869799 0.012220898985253581 0.9877791010147464899 0.010903078202945005 0.989096921797055999 0.00984069902829432 0.9901593009717057 신경망 네트워크 AND AND연산은 입력이 모두 참 값일 때 참이 되고, 그 밖의 경우에는 모두 거짓이 된다. 값을 두 개로 제한하게 되면 아래와 같은 표를 구성할 수 있다. 입력1 입력2 결과 참 참 참 참 거짓 거짓 거짓 참 거짓 거짓 거짓 거짓 식으로 나타내면 아래와 같다. $$ Y=f(X_1 \\times w_1 + X_2 \\times w_2 + 1 \\times b) $$ 다음 코드는 참을 1, 거짓을 0의 정수값으로 설정하고 코드를 작성했다. 12345678910111213141516171819202122232425import mathimport tensorflow as tfimport numpy as npdef sigmoid(x): return 1 / (1 + math.exp(-x))x = np.array([[1,1],[1,0],[0,1],[0,0]])y = np.array([[1],[0],[0],[0]])w = tf.random.normal([2],0,1)b = tf.random.normal([1],0,1)b_x = 1alpha = 0.1for i in range(2000): error_sum = 0 for j in range(4): output = sigmoid(np.sum(x[j]*w)+b_x*b) error = y[j][0] - output w = w + x[j] * alpha * error b = b + b_x * alpha * error error_sum += error if i % 200 == 199: print (i, error_sum) 12345678910199 -0.11417635227874308399 -0.06704566564399024599 -0.047357577895359125799 -0.036500511856403565999 -0.029636905412076681199 -0.0249170597496768121399 -0.0214763375558961431599 -0.0188622308823781331799 -0.0168069230602205561999 -0.015152206075212815 기타 Sigmoid ReLU Gradient Descent 원핫인코딩(one-hot encoding) 컴퓨터는 사칙연산을 빠르게 하는 계산기이므로 계산이 가능한 형태로 변환해야 한다. 원핫인코딩은 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 데이터의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식이다. 이렇게 표현된 벡터를 원핫벡터(One-hot vector)라고 합니다. 워드임베딩을 예로 든다면 다음과 같다. 단어 1 2 3 발 1 0 0 손 0 1 0 머리 0 0 1 window 윈도우(window)란 학습을 하거나 검색을 할 때 몇 개의 데이터를 가지고 하는지를 나타내게 된다. CNN 등에서 윈도우를 n ((\\times)) m 으로 정해서 계속 이동하면서 학습데이터를 생성하는 과정 등이 있다. 우도(like-hood) 모델에서 어떤 결과가 주어졌다 할 때, 그러한 결과가 나올 정도는 얼마나 되는지 나타내는 척도를 말한다. 우도를 측정하기 위해 Bayes’ theorem를 이용한다. cross-entropy 이진분류에서 어떤 데이터가 0이나 1로 예측될 확률은 \\(\\hat y\\), \\(1− \\hat y\\) 이다. 수식으로 정리하여 최대도 만들려는 likehood는 다음과 같이 나타낼 수 있다. $$\\hat y^y (1− \\hat y)(1−y) $$ 여기서 로그를 적용하여 \\( \\log(\\hat y^y)\\)는 \\(y \\log {\\hat y}\\)이고 \\( \\log(1-\\hat y)^{(1-y)}\\)이다. 우도(likehood)를 최대화하기 위한 수식은 \\((- y \\log \\hat y - (1-y) \\log (1-\\hat y))\\)를 최소화하는 것과 같다. 이것은 cross-entropy와 동일한 수식이 되며 cross-entropy를 최소화하는 것은 log-likelihood를 최대화하는 것과 같기 때문에 log loss로 불리기도 한다. 참고자료 하가사나카 류이치로, 아무것도 모르고 시작하는 인공지능 첫걸음, 한빛미디어, 2018. 백성민, 시작하세요! 텐서플로우 2.0 프로그래밍, 위키북스, 2020 원준, 딥러닝을 이용한 자연어 처리 입문, Wikidocs. https://wikidocs.net/book/2155 Lili Jiang, What’s an intuitive way to think of cross entropy?, Quora, 2019. 다른 단어를 잘못 들은 경우 ↩︎ 못 듣고 놓친 경우 ↩︎ 없는 단어를 들었다고 착각한 경우 ↩︎ Vinod Nair and Geoffrey E. Hinton, Rectified Linear Units Improve Restricted Boltzmann Machines , 2010 ↩︎","link":"/Artificial%20Intelligence/"},{"title":"Natural Langauge Process","text":"정의 자연어(Natural Language)란 일반 사회에서 자연히 발생하여 쓰이는 언어[1]를 말한다. 즉, 우리가 일상생활에서 사용하는 언어가 자연어가 된다. 자연어 처리(Natural Language Process)는 글을 쓰거나 대화를 주고받는 등 자연어를 사용하여 컴퓨터가 일을 처리하는 것을 언어처리라고 한다. 최근 기계 번역의 정확도가 높아지거나 사람과 대화를 주고받는 등 기술력이 향상되고 있는 분야이기도 한다. 활용 서비스 문서검색 : 사용자의 검색 쿼리를 받아서 관련 문서를 찾아주는 서비스 기계번역 : 어떤 언어의 텍스트를 다른 언어로 변환하는 서비스 자동요약 : 어떤 문서의 내용을 중요한 내용만 선별하여 요약문을 생성해주는 서비스 자동상담 : 챗봇 혹은 음성으로 사용자의 문의사항을 자동으로 처리하는 서비스 이외에도 인간의 대화방식이나 언어를 모방하여 일을 처리할 수 있는 많은 분야에서 활동되고 있다. 글자레벨에서의 검색 기본적인 검색방법 가장 단순하게 할 수 있는 검색방법은 문장의 왼쪽부터 한 글자씩 맞춰가는 방식이다. 보통 ctrl + F를 사용하는 방식과 유사하다고 생각할 수 있다. 예를 들어 &quot;자연어처리는 무엇인가?&quot;에서 &quot;처리&quot;를 검색한다고 해보면 아래와 같이 된다. 1 2 3 4 5 6 7 8 9 10 11 12 비고 자 연 어 처 리 는 무 엇 인 가 ? 원문 처 리 1번 처 리 2번 처 리 3번 처 리 4번 총 4번으로 해당하는 단어의 위치를 찾아낼 수 있다. 하지만 이 방법은 효율이 굉장히 낮다. 예시처럼 검색 대상 단어가 짧으면 상관없지만 길어지면 비교해야 하는 회수가 늘어나게 된다. Boyer-Moore 알고리즘 1977년에 R.S. Boyer와 J.S. Moore가 만든 알고리즘[2]으로 찾고자 하는 문자열의 마지막 자리가 맞는지 검사하면서 검색하는 방식으로 진행된다. 글자 이동글자수 리 0 처 1 그외 2 1 2 3 4 5 6 7 8 9 10 11 12 비고 자 연 어 처 리 는 무 엇 인 가 ? 원문 처 리 '연’이 처리에 없으므로 문자열 길이만큼 이동 처 리 '처’가 '처리’에 있으므로 '리’를 찾을 수 있는 만큼 이동 처 리 '처리’라는 단어를 찾음 사전검색 검색 문자열이 사전의 어디에 위치하는지를 찾아내는 처리를 말하며 여기서 사전은 표제어 목록을 말한다. 여기서 표제어란 &quot;책이나 장부 가운데 어떤 한 항목을 찾기 쉽게 설정한 말&quot;[3]로 대표어라고도 한다. 하지만 사전의 표제어는 어마어마한 양(위키피디아 한국어판은 표제어만 40만개가 넘고 영어는 500만개가 넘음)이므로 전체 표제어를 하나하나 비교하는 건 비효율적인 방법이다. 이런 과제를 해결하기 위해 고안한 방법이 이진검색(Binary Search)[4]와 트라이(Trie)[5]가 있다. 이진검색 이진검색은 정렬된 배열에서 특정한 값의 위치를 찾는 알고리즘이다. 처음 중간 위치의 값을 선택하고 그 값의 크기를 비교하면서 찾아가게 된다. 단, 검색시 정렬된 배열만 가능하다는 단점이 있지만 검색 프로세스를 반복할 때마다 값을 찾응ㄹ 수 있는 확률이 두 배가 되기 때문에 속도가 빠르다는 장점을 가지고 있다. 예를 들어 정렬된 배열에서 4를 찾는다고 한다면 아래와 같다. 1 2 3 4 5 60 72 83 99 101 113 비고 4 전체에서 중간인 72가 4보다 크므로 왼쪽으로 이동 4 X X X X X 1~60의 중간인 3이 4보다 작으므로 오른쪽으로 이동 X X X 4 X X X X X 4~60의 중간인 5이 4보다 크므로 왼쪽으로 이동 X X X 4 X X X X X X X 값 찾음 트라이 사전을 트리구조로 만드는 것과 같다고 생각하면 된다. 아래 그림처럼 표제어를 단어노드를 사용해서 연결하여 구성된다. 이렇게 구성된 트리를 따라가면서 사전을 탐색하게 되는데 검색이 한번에 끝나기 때문에 매우 빠르게 진행됩니다. 단어레벨의 검색 컴퓨터가 문장을 이해하기 위해서 먼저 단어를 파악하고 단어의 품사를 판정해야 한다. 단어의 품사는 &quot;단어를 기능, 형태, 의미에 따라 나눈 갈래&quot;로 한국어에서는 명사, 대명사, 수사, 조사, 동사, 형용사, 관형사, 부사, 감탄사의 아홉가지로 분류된다. 형태소분석방식은 규칙기반, 통계기반, 딥러닝기반으로 나누어 볼 수 있다. 형태소 분석 형태소란 의미가 있는 최소의 단위로 문법적, 관계적인 뜻을 나타내는 단어 또는 단어의 부분을 말한다. 단어의 품사를 판정하기 위해선 형태소 단위로 문장을 분리해야 하는데 이 과정을 형태소분석이라고 한다. 자립성에 따른 형태소 분류 분류 비고 자립형태소 하늘 매우 혼자쓰임 의존형태소 이 푸르 다 붙어쓰임 의미에 따른 형태소 분류 분류 비고 실질형태소 하늘 매우 푸르 실제 의미 형식형태소 이 다 문법적 의미 형태소분석 방식 분류 내용 단점 규칙기반 자연어가 가지는 규칙과 특징을 사전으로 구축하여 패턴을 미리 정의해놓음 사전 구축에 많은 리소스가 필요하며 모호한 규칙에 대해 대응하기 어려움 통계기반 말뭉치를 이용하여 대용량 텍스트에 나타나는 언어의 현상들을 일반화하는 통계를 활용하며 일반적인 자연어처리에 사용되고 있음 사용하는 말뭉치에 따라 성능/품질이 달라지기 때문에 대용량 말뭉치가 필요함 관련 용어 말뭉치 일상 대화를 기록해 둔 자료부터 신문기사, 소설 등 문자로 작성된 모든 것을 포괄하는 텍스트를 모아놓은 것으로 다음과 같은 특성을 가진다. 텍스트 수집이나 입력 과정에서 원래의 내용이나 형태의 누락이 있어서는 안되며, 원형을 유지하고 보장해야 한다. 언어의 특성을 잘 반영할 수 있는 구성으로 언어의 다양한 변이를 담아야 한다. 해당 언어의 통계적 대표성을 지녀야 하며 유의미한 규모로 확보되어야 한다. 대표적으로 21세기 세종 계획의 결과로 구축된 말뭉치 중에는 현대국어 구어 전사 말뭉치, 한영/한일 병렬 말뭉치, 북한 및 해외 한국어 말뭉치 등이 포함되어 있다. 말뭉치가 있으면 언어처리에 활용할 정확한 자료가 마련되지만 이러한 대규모 말뭉치를 구축하려면 상당히 많은 비용이 들어가게 되는 한계점을 가지고 있다. Term 정규화된 단어 혹은 글자(대문자, 형태, 철자 등)을 의미하며 상황에 따라선 단어와 동일한 레벨로 사용하기도 한다. 하늘 -&gt; 하,늘,하늘 Token 유용한 의미적 단위로 함께 모여지는 일련의 문자열로 구분 기호 사이의 글자 순서를 가진다. 하늘이 매우 푸르다 -&gt; 하늘이,매우,푸르다 N-Gram N개의 문자열 크기 만큼 잘라서 문자열을 왼쪽에서 오른쪽으로 한 개씩 움직이며 추출되는 문자열의 집합 하늘이 매우 푸르다 -&gt; 2-gram -&gt; 하늘,늘이,이_,매,매우,우,_푸,푸르,르다 조사 어미 선어말어미 어휘형태소 문법형태소 용언 제언 접미사 전성어미 어근부 용언화 접미사 서술격 조사 일반적인 형태소분석 형태소 분석의 일반적인 순서는 아래와 같다. 전처리 입력 문자열로부터 문장 부호, 숫자, 영문자와 같이 형태소 분석 대상이 되지 않는 문자들을 분리하거나 제거하게 된다. 일반적으로 단어를 추출할 때 문장부호는 별개의 단어로 독립시키고 불필요한 문자(제어문자,특수문자 등)는 분석이 되지 않도록 공백으로 대치한다. 나는~~ 학교 간다 ^^ -&gt; 나는 학교 간다 문법 형태소 분리 단어를 구성하고 있는 각 형태소들을 분리하는 단계로 한국어의 단어형성규칙에 따라 형태소인 조사와 어미를 분리하게 된다. 어미가 있는 경우 선어말어미를 분리하고 어휘형태소와 문법형태소간의 결합상태를 검사하게 된다. 어미가 분리되어 용언으로 추정되는 단어에 대해서 불규칙 활용이 일어났을 가능성이 있는 경우 불규칙 원형 복원에 의해 원형을 추정하게 된다. 나는 학교 간다 -&gt; 나/는 /학교/ /간/다 체언분석 형태소 분리 결과 조사가 분리된 단어는 체언으로 추정한다. 어근부가 체언이 아닌 경우 접미사를 분리하고 체언 분리를 시도하게 된다. 명사형 전성어미가 발견된 단어는 용언 분석 단계에서 처리하게 된다. 나/는 /학교/ /간/다 -&gt; 나/대명사 는/조사 용언분석 형태소 분리 결과 어미가 분리된 단어는 용언으로 추정하게 되며 어휘형태소 사전에서 용언 분석을 시도한다. 어근부가 용언이 아닌 경우에는 용언화 접미사를 분리하여 '-하다/되다/시키다’와 같은 접미사가 발견되면 체언분석을 시도하게 된다. 서술격 조사 '이’가 발견되거나 서술격 조사가 생략된 경우에도 체언분석을 시도한다. 본 용언과 보조 용언이 결합된 유형이 있는 경우 보조 용언을 먼저 분석하고 본 용언을 분석하게 된다. 나/는 /학교/ /간/다 -&gt; 날/동사 + 는/어미, 가/동사+ㄴ다/어미, 갈/동사+ㄴ다/어미 단일 형태소 분석 형태소 분리 단계에서 조사나 어미가 발견되지 않은 단어는 단일 형태소로 추정하며 어휘형태소를 사전에서 체언, 부사어, 감탄어, 관형어인지 확인한다. 접미사가 분리되는 경우 접미사를 분리하고 어휘형태소 사전을 확인하게 된다. 나/는 /학교/ /간/다 -&gt; 학교/일반명사 복합명사 및 미등록어 추정 조사가 분리되어 체언이 추정된 어근부가 체언으로 분석되지 않는 단어는 어근부가 복합명사로 인식될 수 있는지 확인한다. 또한 분석단계에서 분석되지 않은 단어는 단어형성 규칙을 바탕으로 미등록어를 추정하게 된다. 준말처리 준말에 대해서는 본딧말 규칙이나 기분석 사전을 통해 처리하게 된다. SSD -&gt; Solid State Drive 후처리 영문자나 숫자 등 한글 이외의 문자열에 대해서 출력이 필요하면 연결하여 출력하게 되며 모호성 해결과 같은 형태소 분석의 정확도를 높이기 위한 작업을 진행한다. 한국어 형태소분석의 어려움[6] 용언의 불규칙 활용, 축약, 모음탈락 등 형태적 변형요인이 다양하다. 나는 집에 간다. -&gt; 나 집에 간다. 어휘형태소와 문법형태소의 결합과정에서 형태적 변형이 발생한다. 형태소만 가지고는 정의가 안되는 것들이 많다. 비가 온다 -&gt; 가수 비? 하늘에서 내리는 비? 조사나 어미는 음소단위로 결합되기 때문에 형태 분석을 위해서는 음절단위의 글자를 음소단위의 자소로 나누어야 한다. 간다 -&gt; 가+ㄴ다. 통계기반 형태소분석 예를 들어 &quot;검은콩&quot;를 분석하게 되면 &quot;검은/형용사 콩/명사&quot;와 &quot;검은콩/명사&quot;라는 후보 중 많이 나온 형태가 &quot;검은콩/명사&quot;라면 후보 선택으로 &quot;검은콩/명사&quot;를 출력하게 되는 방식을 말한다. 통계적인 형태소분석을 사용하기 위해서는 말뭉치(corpus)가 필요하게 되는데 예를 들면 다음과 같다. [검정/명사 고무신/명사] [2019/숫자 년/명사] [임용고시/명사] 이렇게 통계를 사용하다보면 제로 빈도 문제[7]가 발생하게 되는데 이는 긴 문장이나 보기 드문 단어가 들어간 단어 열에 대한 말뭉치 내의 빈도가 0이어서 확률을 제대로 계산하지 못하는 문제를 말한다. 이러한 경우는 품사를 제대로 판정하기 어렵고 이에 대한 처리가 반드시 필요하게 된다. 딥러닝기반 형태소분석 카카오형태소분석기 Khaiii https://github.com/kakao/khaiii 임베딩(Embedding) 컴퓨터는 자연어를 사람처럼 이해할 수 없으며 임베딩을 활용하여 자연어를 계산하는 것이 가능하다. 임베딩은 자연어를 컴퓨터가 처리할 수 있는 숫자들의 나열인 벡터로 바꾼 결과이다. 화자가 얘기하는 것 중 중요한 부분은 세가지로 나눠볼 수 있다고 가정한다. 문장에 어떤 단어가 많이 쓰였는가? 단어가 어떤 순서로 등장하는가? 문장에 어떤 단어가 같이 나타났는가? 백오브워즈(Bag of Words) 백오브워즈에서는 어떤 단어가 많이 사용되었는지가 중요하고 단어의 순서는 무시하게 된다. 백워브워즈는 문장을 형태소분석을 통해 단어들로 나누고 모든 단어의 유무를 나타낼 수 있는 배열에 넣어 빈도를 나타내게 된다. 소비가 줄고 경기가 얼어붙자, 일자리도 빠르게 사라졌습니다. 3월의 고용 통계를 보면 지난해 같은 달에 비해서 사업체에서 일하는 사람의 숫자가 줄었습니다. 지난 3월 국내 사업자 종자사 수는 지난해 같은 달과 비교해 22만5000명 줄었습니다. 2009년 관련 통계를 처음 낸 이후 11년 만에 첫 마이너스입니다. 상대적으로 고용구조가 취약한 일자리가 큰 타격을 받았습니다. 국내 달 비교 통계 처음 마이너스 일자리 … 1 2 1 1 1 1 2 … 백오브워즈에서는 단어의 빈도가 비슷하면 관련성이 상대적으로 높을 것이라 추정하며 정보검색에서는 사용자의 질의에 가장 적절한 문서를 보여줄 때 질의를 백오브워즈로 변환하고 대상 문서에서 빈도 통계상 유사도가 높은 문서를 찾아주게 된다. TF-IDF(Term Frequency-Inverse Document Frequency) 예를 들어 달/월 등 어떤 문서에서 단어가 많이 나온 것만 가지고 유사하다고 보기 어려운 경우가 많기 때문에 이러한 단점을 극복하기 위해 제안되었다. $$ TF-IDF(w) = TF(w) \\times log( \\frac{N}{DF(w)}) $$ 수식에서 \\(TF\\)는 특정 문서에서 특정단어가 나타난 수를 나타내고 \\(DF\\)는 대상 문서 모두에서 특정 단어가 나타는 수를 나타낸다. 여기서 수식에 의해 \\(IDF\\)는 값이 클수록 특이한 단어라는 걸 나타내게 된다. 즉, 어떤 단어의 특이성이 높아질수록 가중치가 높아지게 되어 문서의 주제를 예측하기 쉬워진다. Deep Averaging Network 백워브워즈의 머신러닝 기법으로 백워브워즈처럼 단어를 순서를 고려하지 않고 어떤 단어가 얼마나 많이 사용하고 있는지에 따라 문서를 분류한다. 간단한 구조의 아키텍쳐임에도 성능이 좋아서 많이 사용되고 있다. $$ V = \\frac {1}{n} \\times \\sum_{i=1}^{n} e^i $$ NPLM(Neural Probabilistic Language Model) 요슈아 벤지오 연구팀이 2003년도에 제안한 방법으로 학습 데이터에 한 번도 등장하지 않는 단어(혹은 N-Gram)가 등장했을 때 문제가 발생할 수 있는 등의 전통적인 언어 모델의 단점을 극복하고자 했다. 당시에는 NPLM으로 발표 했으며 현재는 Neural Network Language Model(Feed Forward Neural Network Language Model,NNLM)로도 불린다. 베지오 연구팀은 기존 언어 모델의 단점을 다음과 같이 정의하였다. 학습 데이터에 존재하지 않는 단어(n-gram)이 포함된 단어가 나타날 수 있다. 과거의 정보가 마지막까지 전달되지 않는 장기 의존성(long-term dependency)을 포착하기 어렵다. 단어/문장 간 유사도를 계산할 수 없다. NPLM은 직전까지 등장한 n-1 개 단어들로 다음 단어를 맞추는 n-gram 언어모델이며 수식은 다음과 같다. $$ P(w_t|w_{t-1},…,w_{t-n+1}) = \\frac{exp(Y_{w_t})}{\\sum_i exp(Y_i)} \\\\ w_t : t번째 단어 , Y_{w_t} : 출력 $$ NPLM은 Input Layer(입력층), Projection Layer(투사층), Hidden Layer(은닉층), Output Layer(출력층)으로 이루어져 있다. 입력단계 입력단계는 입력층에서 투사층까지의 처리를 하게 된다. $$ X_t = C \\cdot w_t = C(w_t)$$ 입력은 문장 내 t번째 단어(\\(w_t\\))에 대응하는 단어 벡터 \\(X_t\\)를 만드는 과정으로 행렬 \\(C\\)와 \\(w_t\\)에 해당하는 원핫벡터[8]를 한 것과 같다. \\(C\\)라는 행렬에서 \\(w_t\\)에 해당하는 행만 가져오게 되며 이렇게 만들어진 테이블을 참조(lookup)테이블이라고 한다. 예를 들어 \\( \\begin{bmatrix} 1&amp;2&amp;3 \\cr 4&amp;5&amp;6 \\cr 7&amp;8&amp;9 \\end{bmatrix} \\) 배열에 대해 3번째 값만 참조한다면 다음과 같이 표현할 수 있다. $$ C(w_3) = \\begin{bmatrix} 0&amp;0&amp;1 \\end{bmatrix} \\times \\begin{bmatrix} 1&amp;2&amp;3 \\cr 4&amp;5&amp;6 \\cr 7&amp;8&amp;9 \\end{bmatrix} = \\begin{bmatrix} 7&amp;8&amp;9 \\end{bmatrix} $$ 예를 들어 “여름 아이스크림 벤치 시원 기분”(여름에 아이스크림을 벤치에서 먹으면 시원하고 기분이 좋다) 에서 &quot;아이스크림&quot;를 예측한다고 한다면 다음과 같이 진행된다. 단어레벨에서 진행되기 때문에 조사 등은 생략하고 단어로만 예시를 들었음 원핫벡터를 생성하게 되면 아래와 같이 된다. $$여름 = \\begin{bmatrix} 1&amp;0&amp;0&amp;0&amp;0 \\end{bmatrix} \\\\ 아이스크림 = \\begin{bmatrix} 0&amp;1&amp;0&amp;0&amp;0 \\end{bmatrix} \\\\ 시원 = \\begin{bmatrix} 0&amp;0&amp;1&amp;0&amp;0 \\end{bmatrix} \\\\ 벤치 = \\begin{bmatrix} 0&amp;0&amp;0&amp;1&amp;0 \\end{bmatrix} \\\\ 기분 = \\begin{bmatrix} 0&amp;0&amp;0&amp;0&amp;1 \\end{bmatrix}$$ 여기서는 가중치 벡터 \\(w_t\\)는 투사층의 크기(m)를 3으로 설정하게 되면 아래와 같다고 가정하자. $$ w_t = \\begin{bmatrix} 0.1&amp;0.2&amp;0.3 \\cr 0.4&amp;0.5&amp;0.6 \\cr 0.7&amp;0.8&amp;0.9 \\cr 1.0&amp;1.1&amp;1.2 \\cr 1.3&amp;1.4&amp;1.5 \\end{bmatrix} $$ 여기서 &quot;시원&quot;일 경우 원핫벡터를 생성하면 아래와 같이 계산된다. $$ C(w_{시원}) = \\begin{bmatrix} 0&amp;1&amp;0&amp;0 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.1&amp;0.2&amp;0.3 \\cr 0.4&amp;0.5&amp;0.6 \\cr 0.7&amp;0.8&amp;0.9 \\cr 1.0&amp;1.1&amp;1.2 \\cr 1.3&amp;1.4&amp;1.5 \\end{bmatrix} = \\begin{bmatrix} 0.4&amp;0.5&amp;0.6 \\end{bmatrix} $$ 출력단계 출력단계는 투사층에서 은닉층을 거쳐 출력층으로 이루어지게 된다. NPLM 수식에서 \\(Y\\)는 말뭉치 전체의 단어수(V)에 해당하는 차원을 가진 스코어 벡터이다. 예를 들어 전체 단어수가 3이면 다음과 같이 나타낼 수 있다. $$ Y_{w_t} = \\begin{bmatrix} 1 \\cr 2 \\cr 3 \\cr \\end{bmatrix} , V=3 $$ 결국 출력은 V차원의 스코어 벡터에 softmax함수를 적용한 V차원의 확률벡터이다. NPLM은 확률값이 가장 높은 단어의 인덱스의 단어가 실제 정답 단어와 일치하도록 학습이 진행된다. $$ Y_{w_t} = b + U \\cdot \\tanh (d+H_{x_t}) \\\\ b,d : bias벡터\\\\ H : 입력층에서 은닉층으로 보내주는 가중치 매트릭스\\\\ U : 은닉층에서 출력층으로 보내주는 가중치 매트릭스 $$ 앞의 예를 다시 가져오게 되면 아래와 같이 된다. $$ Y_{w_t} = \\begin{bmatrix} 0.12 \\cr 0.7 \\cr 0.23 \\cr 0.3 \\cr 0.13 \\end{bmatrix} = \\begin{bmatrix} 0 \\cr 1 \\cr 0 \\cr 0 \\cr 0 \\end{bmatrix} = \\begin{bmatrix} 여름 \\cr 아이스크림 \\cr 벤치 \\cr 시원 \\cr 기분 \\end{bmatrix} = 아이스크림 $$ 이런 방식을 통해 단어들의 순서에 의해 이전 단어를 통해 단어를 예측할 수 있게 된다. Word2Vec 2013년 구글 연구팀이 &quot;Efficient Estimation of Word Representations in Vector Space(Mikolov et al., 2013a)[9]&quot;과 &quot;Distributed Representation of Words and Phrase and their Compositionality(Mikolov et al., 2013b)[10]&quot;를 발표하여 가장 널리 쓰이고 있는 단어레벨의 임베딩 모델이다. &quot;2013a&quot;에서는 Skip-Gram과 CROW(Continuous Bag of Words)가 제안되었고 &quot;2013b&quot;에서는 두 모델을 기반으로 하여 네거티브 샘플링[11] 등 학습 최적화 기법을 제안한 내용이 핵심 골자이다. CROW Skip-gram CROW는 주변에 있는 단어(문맥단어, context word)들을 가지고 알고자하는 단어(중심단어, 타깃단어, center word, target word)를 맞추는 과정으로 학습하고 Skip-gram은 중심단어를 가지고 주변의 문맥단어를 무엇일지 예측하는 과정으로 학습하게 된다. 단순히 학습에 사용되는 데이터쌍으로 본다면 CROW는 4:1(w1~4:W)인 반면 Skip-gram은 1:1 * 4(W:w1, W:w2, W:w3, W:w4)로 더 많은 학습데이터쌍을 확보할 수 있어 일반적으로 품질이 좋은 편이다. CROW “여름 아이스크림 벤치 시원 기분”(여름에 아이스크림을 벤치에서 먹으면 시원하고 기분이 좋다) 에서 &quot;아이스크림&quot;를 예측한다고 한다고 하고 정답을 \\(y\\)라고 설정하고 학습과정을 시작해보자. 전체 크기가 \\(m\\) 인 입력 데이터를 원핫벡터 (\\(x^{(c-m)} ,…, x^{(c-1)} , x^{(c+1)} ,…, x^{(c+m)}\\)) 로 변경하면 아래와 같이 된다. 여기서 \\(c\\)는 중심단어의 위치를 나타낸다. $$x^{여름,c-1} = \\begin{bmatrix} 1&amp;0&amp;0&amp;0 \\end{bmatrix} \\\\ x^{시원,c+1} = \\begin{bmatrix} 0&amp;1&amp;0&amp;0 \\end{bmatrix} \\\\ x^{벤치,c+2} = \\begin{bmatrix} 0&amp;0&amp;1&amp;0 \\end{bmatrix} \\\\ x^{기분,c+3} = \\begin{bmatrix} 0&amp;0&amp;0&amp;1 \\end{bmatrix}$$ \\(V\\)는 입력워드행렬(input word matrix)로 \\(V\\)의 \\(i\\)번째 열이 임베디드 벡터인 \\(x^i\\)를 입력되게 되며 입력레이어의 데이터가 된다. $$ v_{c−m}=Vx^{(c−m)} , v_{c−m+1}=Vx^{(c−m+1)} ,…, v_{c+m}=Vx^{(c+m)}$$ 입력워드행렬을에 대한 평균을 내게 된다. $$ \\hat v = \\frac{(v_{c−m}=Vx^{(c−m)} , v_{c−m+1}=Vx^{(c−m+1)} ,…, v_{c+m}=Vx^{(c+m)})}{2m} $$ \\(U\\)는 출력워드행렬(output word matrix)로 \\(U\\)의 \\(j\\)번째 열이 n차원의 임베디드 벡터로 표현된다. 이를 활용하여 스코어를 구하면 아래와 같이 된다. $$ z = U \\hat v $$ 스코어를 확률로 변경한다. $$ \\hat y = softmax(z) $$ \\( \\hat y \\)가 정답인 \\(y\\)와 같아지도록 반복 학습시켜야 한다. \\(U\\)의 \\(j\\)번째 열을 \\(U_j\\)로 표기할 때, \\( z_j = U_j \\cdot \\hat{v}\\)가 크면 클수록,즉 \\(U_j\\)와 \\(\\hat{v}\\)가 비슷한 벡터일수록 \\(x^j\\)가 중심단어로 채택될 확률이 높아진다. Skip-gram 포지티브 샘플은 중심 단어 주변에 등장한 단어를 말하며 네거티브 샘플은 주변에 등장하지 않은 단어의 쌍을 말한다. CROW의 예제를 다시 활용하고 윈도우[12]를 2로 설정한다면: \\( t: target word , c : context word \\) 포지티브 샘플은 아래와 같다. t c 아이스크림 여름 아이스크림 시원 네거티브 샘플은 다음과 같다. t c 아이스크림 전자렌지 아이스크림 가을 아이스크림 분노 Skip-gram모델은 CROW와 비슷하긴 하지만 다음과 같이 진행되게 된다. 원핫벡터로 입력워드행렬 \\(x\\)를 만든다. 임베이드 벡터인 \\(vc = Vx\\)를 만든다. 평균이 아니라 \\( \\hat v = vc \\)를 사용한다. \\(u = Uv_c\\)를 사용해서 스코어 벡터(\\(u_{c−m} , u_{c−m+1} ,…, u_{c+m})\\)를 만든다. 각 스코어를 확률로 변환한다. $$ y = softmax(u) $$ CROW에서도 사용할 수 있지만 모델을 평가하기 위한 손실함수(loss function, objective function)은 아래와 같이 사용할 수 있다. $$ minimize J = - \\log P(w_{c-m},…,w_{c-1},w_{c+1},…w_{c+m}|w_c) \\\\ = - \\log \\prod_{j=0, j \\neq m}^{2m} P(w_{c-m+j}|w_c) \\\\ = - \\log \\prod_{j=0, j \\neq m}^{2m} P(u_{c-m+j}|v_c) \\\\ = - \\log \\prod_{j=0, j \\neq m}^{2m} \\frac{\\exp(u_{c-m+j}^T)v_c}{ \\sum_{k=1}^{|W|} \\exp(u_k^Tv_c)} \\\\ = - \\sum_{j=0, j \\neq m}^{2m} u_{c-m+j}^T + 2m \\log \\sum_{k=1}^{|W|} \\exp(u_k^Tv_c) \\\\ $$ 각 단계마다 unknown 파라미터들에 대한 기울기를 계산할 수 있고, 각 단계마다 SGD(Stochastic Gradient Descent)를 이용하여 파라미터들을 업데이트 할 수 있다. 수식으로 표현하면, $$ J = - \\sum_{j=0, j \\neq m}^{2m} \\log P(u_{c-m+j}|v_c) \\\\ = \\sum_{j=0, j \\neq m}^{2m} H(\\hat y, y_{c-m+j}) $$ \\(H(\\hat{y},y_{c-m+j}\\))는 probability vector \\(\\hat{y}\\)와 one-hot vector \\(y_{c-m+j}\\)간의 cross-entropy[13]이다. 손실함수의 값을 최대한 작게 만들어야 품질이 좋아지게 되는데 전체 단어 집합인 |W|에 대해 모두 계산해야 한다. 보통 이런 단어 집합은 수십만개나 되고 손실함수가 바뀔 때마다 모두 계산을 해야 하므로 매우 비효율적이다. 이 부분을 최소화하기 위해 &quot;2013b&quot;에서 Hirerarchical Softmax와 네거티브 샘플링(Negative Sampling)이 제시되었다. 두 가지 방법을 각각 적용하여 성능을 평가한 결과 네거티브 샘플링의 성능이 좋았고 CROW보다 Skip-gram으로 적용했을 때 성능이 더 좋았다. 따라서 이후에는 Skip-gram에 네거티브 샘플링을 적용한 방법이 일반적으로 많이 쓰이게 되었고 SGNS(Skip-Gram with Negative Sampling)이라는 말로 word2vec을 대체하기도 했다. 2014년 발표된 Glove와 함께 SGNS는 단어레벨 임베딩의 양대 산맥으로써 자리잡았다. Hirerarchical Softmax 모든 단어가 나온 횟수를 센 다음 이진 트리를 구성하여 트리의 최초 단어(root)에는 말뭉치 상에서 가장 높은 빈도로 등장한 단어를, 그 자식 노드에는 2,3번째로 등장한 단어를, 각각의 다음 자식 노드에는 그 다음으로 많이 등장한 단어를 지정하는 방식으로 구성한다. 이 트리는 softmax 함수의 산출값을 구할 때 루트에서부터 내가 확률값을 구하고자 하는 단어가 저장된 리프(leaf) 노드까지 가는 길에 저장된 확률을 곱해나가는 식으로 이를 계산할 수 있도록 도와줍니다. 즉, 이 트리를 사용하면 임의의 어휘 벡터에 대한 softmax 산출값을 구하기 위해 모두 탐색하지 않고 적은 횟수로 softmax값을 구할 수 있다. Negative Sampling 어떤 말뭉치에서 등장한 어휘가 n개라면 특정 맥락에서 나와야 하는 단어 몇 개, 나오면 안되는 단어 몇 개만 추려서 계산해보자는 관점에서 출발하였다. 네거티브 샘플링 방식으로 학습하게 되면 1개의 포지티브 샘플과 k개의 네거티브 샘플만 계산하면 되기 때문에 모델을 단계마다 전체 단어를 모두 계산하는 방식보다는 빠르게 진행된다. 예를 들어 “여름 아이스크림 벤치 시원 기분”(여름에 아이스크림을 벤치에서 먹으면 시원하고 기분이 좋다)에서 &quot;아이스크림&quot;을 중심단어로 보고 “여름”,&quot;벤치&quot;를 맞추는 모델을 학습한다고 하면 “여름”,&quot;벤치&quot;는 포지티브 샘플이 되고 수십만개의 네가티브 샘플이 생기게 된다. 여기서 네가티브 샘플링할 수를 2개로 제한한다면 {“여름”-“자동차”,“학교”},{“벤치”-“구름”,“비”} 등과 같이 구성하여 간단한 이진 분류형태로 변경할 수 있다. 그런데 여기서 문제가 발생한다. 수십만개의 네가티브 샘플에서 어떤 단어를 뽑아서 쓸 것인가 정하기 어렵다. 이런 부분을 해결하기 위해 Noise Distribution을 정의하고 그 분포를 이용해 단어들을 뽑아서 사용하는데 논문에서는 Unigram Distribution[14]의 3/4승을 이용해서 좋은 결과를 낸 것으로 되어 있다. \\({U(w_i)}\\)은 해당 단어의 Unigram확률로 (해당 단어 수 / 전체 단어 수)으로 나타낼 수 있다. $$P_{negative}(w_i) = { {U(w_i)}^{3/4} \\over {\\sum_{j=0}^n U(w_j)}^{3/4} } $$ TODO:예제와 이진분류형태로 어떻게 변하는지 다시 구성 FastText SGNS로 학습된 단어레벨의 임베딩으로 인해 눈에 띄게 발전하면서 많은 분야에 적용되었다. 이 과정에서 word2vec의 단점이 나타나게 되는데 영어가 아닌 언어에 대해 적용하면 여러가지 문제가 생긴다는 것이었다. 한국어와 같은 교착어는 동일한 단어가 문맥 속에서 문법적 규칙이 복잡하게 적용된다. 의미상 유사하지만 문법적으로 조금만 다르면 서로 전혀 상관없는 단어가 되는 경우가 생기고 이에 대해 어휘에 대해 임베딩을 구성할 방법이 없었다. 이러한 문제를 해결하기 위한 시도가 FastText이다. 2015년부터 텍스트에서 단어보다 낮은 단위의 입력(n-gram 등)을 입력하면 더 좋은 결과를 낸다는 논문들이 나오기 시작했는데 특히 글자 수준(character-level)으로 나누어 학습하면 더 좋은 결과를 낸다는 관점에서 단어레벨 임베딩을 적용한 것이 Facebook AI Research(FAIR)에서 개발한 FastText이다. FastText는 SGNS를 기반으로 하지만 최소 단위가 단어가 아닌 n-gram 단위로 내렸다는 것이다. 구체적으로 단어간 구분을 &lt;,&gt;로 하고 tri-gram에 대한 벡터를 만들게 됩니다. 이러한 방법을 Subword-Information Skip-Gram(SISG)라고 한다. 이러한 방식을 적용하면 n-gram 수준에서 학습하기 때문에 동일한 의미를 갖는 어휘가 문법적인 규칙에 따라서 변화하는 패턴을 학습하기 쉬워진다. 이렇게 하게 되면 적은 양의 데이터를 가지고 더 많은 학습 데이터를 만들 수 있어서 성능이 올라가는 효과가 있다. 또, 학습할 때 존재하지 않았던 어휘(OOV, out of vocabulary)에 대한 임베딩까지 적용할 수 있는데 예를 들면 &quot;군고구마&quot;라는 단어를 학습하지 않았더라도 &quot;군&quot;과 &quot;고구마&quot;라는 n-gram을 학습한 적이 있다면 두 임베딩 벡터를 조합하여 임베딩 벡터를 만들어 낼 수 있기 때문에 생소한 단어에 대해서도 word2vec에 비해 월등한 성능을 보인다고 증명되었다. 한국어 임베딩의 경우에도 word2vec과 비교했을 때 FastText로 학습한 단어레벨 임베딩의 성능이 더 좋다는 것이 확인되었다. GloVe(Global Word Vectors) 2014년 미국 스탠포드대학에서 개발한 임베딩 방법론으로 임베딩된 두 단어 벡터의 내적이 말뭉치 전체에서의 동시 등장확률 로그값이 되도록 목적함수를 정의하였다. 논문에서는 LSA는 말뭉치 전체의 통계적인 정보를 모두 활용하지만 LSA 결과물을 가지고 단어/문서 간 유사도를 측정하기 어렵고 Word2Vec은 사용자가 지정한 윈도우 내에서만 학습/분석이 이뤄지게 때문에 말뭉치 전체의 공기정보(co-occurrence)는 반영하기 어렵다고 밝혔다. GloVe는 LSA의 카운트 기반의 방법과 Word2Vec의 예측 기반의 방법을 모두 사용하여 구현하였다. https://wikidocs.net/22885 https://ratsgo.github.io/from frequency to semantics/2017/04/09/glove/ 윈도우 기반 동시 등장 행렬(Windows based co-occurrence matrix) 단어의 동시 등장 행렬은 행과 열을 전체 단어 집합의 단어들로 구성하고 \\(i\\) 단어의 윈도우[12:1] 크기 내에서 \\(k\\) 단어가 등장한 횟수를 \\(i\\)행 \\(k\\)열에 기재한 행렬을 만한다. 예를 들어 다음과 같은 문장이 있다고 가정하고 윈도우 크기를 1이라고 한다면: 나는 인공지능을 공부한다. 나는 자율주행 자동차가 좋다. 나는 운전이 좋다. 나는 인공지능을 공부한다 자율주행 자동차가 좋다 운전이 나는 0 1 0 1 0 0 1 인공지능을 1 0 1 0 0 0 0 공부한다 0 1 0 0 0 0 0 자율주행 1 0 0 0 1 0 0 자동차가 0 0 0 0 0 1 0 좋다 0 0 0 0 1 0 1 운전이 1 0 0 0 0 1 0 예제는 이해를 돕기 위한 부분이고 실제와 다를 수 있음 동시 등장 확률(co-occurence probability) 동시 등장 확률 \\(P(k|i)\\)는 동시 등장 행렬로부터 특정 단어 \\(i\\)의 전체 등장 횟수를 합하고 특정 단어 \\(i\\)가 등장했을 때 어떤 단어 \\(k\\)가 등장한 횟수를 합하여 계산한 조건부 확률이다. \\(P(k|i)\\)에서 \\(i\\)를 중심단어, \\(k\\)를 주변단어라고 했을 때, 동시 등장 행렬에서 중심단어 \\(i\\)의 행의 모든 값을 더한 값을 분모로 하고 \\(i\\)행 \\(k\\)열의 값을 분자로 한 값이라고 할 수 있다. TODO:확률계산 결과 필요 손실함수 GloVe의 단어 벡터 학습방식은 co-occurrence가 있는 두 단어의 단어 벡터를 이용하여 co-occurrence 값을 예측하는 regression 문제를 풉니다. GloVe는 &quot;임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만드는 것&quot;이다. GloVe의 연구진들은 벡터 \\( w_i \\), \\( w_k \\), \\( \\tilde w_k \\)를 가지고 어떤 함수 \\( F \\)를 수행하면 \\( \\frac {P_ik}{P_jk} \\)가 나온다는 초기 식으로부터 시작하는 걸 제안했다. $$ F(w_i, w_j, \\tilde w_k) = \\frac {P_ik}{P_jk} $$ 함수 F는 두 단어의 사이의 동시 확률의 크기 관계 비율 정보를 벡터 공간에 인코딩하는 것이 목적이며 \\( w_i \\)와 \\( w_j \\)라는 두 벡터의 차이를 함수 \\( F \\)의 입력으로 사용한다. $$ F(w_i - w_j, \\tilde w_k) = \\frac {P_ik}{P_jk} $$ 위 수식대로 보면 우변은 스칼라값이고 좌변은 벡터값이 된다. 이를 맞춰주기 위해 \\( F \\)의 입력에 내적(dot product)를 수행한다. $$ F(( w_i - w_j)^T, \\tilde w_k) = \\frac {P_ik}{P_jk} $$ 여기서 함수 \\(F\\)가 만족해야 할 필수 조건이 있는데 중심 단어 \\(w\\)와 주변 단어 \\( \\tilde w \\)는 무작위 선택이므로 이 둘의 관계는 자유롭게 바뀔 수 있도록 해야 한다. 이것을 성립되게 하기 위해서 함수 \\( F \\)가 실수의 덧셈과 양수의 곱셈에 대해서 준동형(Homomorphism)[15]을 만족하도록 해야 한다. 이를 수식으로 나타내면 아래와 같다. $$ F(a+b) = F(a)F(b), \\forall a,b \\in \\mathbb{R} $$ 이제 GloVe식을 다시 보면 함수 \\(F\\)는 결과값으로 스칼라 값인 \\(\\frac{P_ik}{P_jk}\\)가 나와야 한다. 이렇게 되려면 \\(a\\)와 \\(b\\)가 각각 두 벡터의 내적값이 되어야 한다. $$F(v_1^T v_2 + v_3^T v_4) = F(v_1^T v_2)F(v_3^T v_4), \\forall v_1,v_2,v_3,v_4 \\in V, V:vector $$ 그런데 앞서 작성한 식에서는 \\(w_i\\)와 \\(w_j\\)라는 두 벡터의 차이를 함수 \\(F\\)의 입력으로 받고 이를 준동형식으로 변경하면 아래와 같다. $$F(v_1^T v_2 - v_3^T v_4) = \\frac{F(v_1^T v_2)}{F(v_3^T v_4)}, \\forall v_1,v_2,v_3,v_4 \\in V, V:vector $$ 이제 준동형을 GloVe 식에 적용해보면 아래와 같다. $$F((w_i - w_j)^T \\tilde w_k) = \\frac{F(w_i^T \\tilde w_k)}{F(w_j^T \\tilde w_k)} $$ 여기서 우변은 본래 \\(\\frac{P_ik}{P_jk}\\) 였으므로 다음과 같이 표현될 수 있다. $$ \\frac{P_ik}{P_jk} = \\frac{F(w_i^T \\tilde w_k)}{F(w_j^T \\tilde w_k)} $$ $$ F(w_i^T \\tilde w_k) = P_ik = \\frac {X_ik}{X_i} $$ 이제 함수 \\(F\\)를 찾아야 하는데 이를 정확하게 일치하는 함수가 바로 지수함수이다. \\(F\\)를 지수함수 \\(exp\\)라고 해보자. $$ exp(w_i^T \\tilde w_k - w_j^T \\tilde w_k) = \\frac{exp(w_i^T \\tilde{w_k})}{exp(w_j^T \\tilde{w_k})} $$ $$ exp(w_i^T \\tilde w_k) = P_ik = \\frac{X_ik}{X_i} $$ 위의 두번째 식을 로그함수로 변환하면 아래와 같다. $$ w_i^T \\tilde w_k = log P_ik = log(\\frac{X_ik}{X_i}) = log(X_ik) - log(X_i) $$ 단, 제한사항에서 언급했듯이 \\(w_i\\)와 \\(\\tilde w_k\\)는 두 값의 위치를 바꿔도 식이 성립되어야 한다. 그런데 이게 성립되려면 \\(log(X_i)\\)항이 문제가 된다. 그래서 논문에서는 \\(log(X_i)\\)항을 \\(w_i\\)에 대한 편향 \\(b_i\\)라는 상수항으로 대체하기로 한다. $$w_i^T \\tilde w_k + b_i + \\tilde b_k = log (X_ik) $$ 우변의 값과의 차이를 최소화하는 방향으로 좌변의 4개의 항은 학습을 통해 값이 바뀌는 변수들이 된다. 즉 손실함수는 아래와 같이 정의할 수 있다. $$ Loss function = \\sum_{m,n=1}^V (w_m^T \\tilde w_n + b_m + \\tilde b_n - log X_{mn})^2 $$ 여기서 \\(V\\)는 단어 집합의 크기를 의미한다. 여기까지 정의하고나서도 연구진은 \\(log X_ik\\)에서 \\(X_ik\\)값이 0가 될 수 있음을 지적했고 그 대안 중 하나로 \\(log X_ik \\)항을 \\(log(q+X_ik)\\)로 변경한다. 하지만 여기까지 와도 해결안되는 부분이 있는데 동시 등장 행렬 \\(X\\)는 마치 백오브워즈에서 처럼 희소 행렬일 가능성이 다분하다는 점이다. Swivel(Submatrix-Wise Vector Embedding Learner) https://arxiv.org/pdf/1602.02215.pdf 문장레벨의 검색 Levenshtein Distance(Edit Distance) https://lovit.github.io/nlp/2018/08/28/levenshtein_hangle/ Doc2Vec ELMo BERT 참고자료 이기창, 한국어 임베이딩, 에이콘, 2019. 임희석, 자연어처리 바이블, 휴먼싸이언스, 2019. 하가사나카 류이치로. 아무것도 모르고 시작하는 인공지능 첫걸음. 한빛미디어, 2018. CS224D: Deep Learning for NLP, Stanford, 2016. 원준, 딥러닝을 위한 자연어 처리 입문, Wikidocs, 2020. Noam Shazeer외 3인, Swivel: Improving Embeddings by Noticing What’s Missing, Google, 2016. 자연언어,표준국어대사전(네이버사전) ↩︎ Boyer–Moore string-search algorithm, wikipedia ↩︎ 표제어,고려대한국어사전(네이버사전) ↩︎ Binary Search, wikipedia ↩︎ Trie, wikipedia ↩︎ 형태분석, wikipedia ↩︎ Data Compression/The zero-frequency problem, wikibook ↩︎ 원핫벡터 ↩︎ Efficient Estimation of Word Representations in Vector Space(Mikolov et al., 2013a) ↩︎ Distributed Representation of Words and Phrase and their Compositionality(Mikolov et al., 2013b) ↩︎ 네거티브 샘플링 ↩︎ 윈도우 ↩︎ ↩︎ cross-entropy ↩︎ unigram은 1-gram과 동일하며 unigram-distribution은 전체 데이터를 단어의 unigram으로 생성한 확률분포를 나타낸다. ↩︎ \\(X\\)의 임의의 원소 \\(x\\), \\(y\\)와 연산 \\(*\\) 및 그에 대응되는 \\(Y\\)의 연산 \\( \\circ \\)에 대해, \\(f(x*y) = f(x) \\circ f(y) \\)를 만족한다. \\( log(a \\times b) = log(a) + log(b) \\) 가 대표적인 예이다. 준동형사상 ↩︎","link":"/Natural%20Language%20Process/"}],"tags":[{"name":"ai","slug":"ai","link":"/tags/ai/"},{"name":"인공지능","slug":"인공지능","link":"/tags/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/"},{"name":"AI","slug":"AI","link":"/tags/AI/"},{"name":"자연어처리","slug":"자연어처리","link":"/tags/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"}],"categories":[{"name":"technology","slug":"technology","link":"/categories/technology/"}]}