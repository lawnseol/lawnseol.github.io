<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.0"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Tag: 자연어처리 - Knowledges for life</title><meta description="I love technology to improve people&amp;#39;s lives, and harmonious architecture with nature."><meta property="og:type" content="blog"><meta property="og:title" content="Knowledges for life"><meta property="og:url" content="https://lawnseol.github.io/"><meta property="og:site_name" content="Knowledges for life"><meta property="og:description" content="I love technology to improve people&amp;#39;s lives, and harmonious architecture with nature."><meta property="og:locale" content="en_US"><meta property="og:image" content="https://lawnseol.github.io/img/og_image.png"><meta property="article:author" content="Lawn Seol"><meta property="article:tag" content="AI"><meta property="article:tag" content=" DeepLearning"><meta property="article:tag" content=" Computer Science"><meta property="article:tag" content=" Tea"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://lawnseol.github.io"},"headline":"Knowledges for life","image":["https://lawnseol.github.io/img/og_image.png"],"author":{"@type":"Person","name":"Lawn Seol"},"description":"I love technology to improve people&#39;s lives, and harmonious architecture with nature."}</script><link rel="alternative" href="/rss2.xml" title="Knowledges for life" type="application/atom+xml"><link rel="icon" href="https://www.gravatar.com/avatar/ce8b6bf07418a33af34ecc50383675a7?s=128"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/monokai.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-88598623-3" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-88598623-3');</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="https://www.gravatar.com/avatar/ce8b6bf07418a33af34ecc50383675a7?s=128" alt="Knowledges for life" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/lawnseol"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">자연어처리</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-05-12T12:59:13.613Z" title="2020-05-12T12:59:13.613Z">2020-05-12</time><span class="level-item"><a class="link-muted" href="/categories/technology/">technology</a></span><span class="level-item">an hour read (About 10874 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Natural%20Language%20Process/">Natural Langauge Process</a></h1><div class="content"><h2 id="정의">정의</h2>
<p>자연어(Natural Language)란 일반 사회에서 자연히 발생하여 쓰이는 언어<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>를 말한다. 즉, 우리가 일상생활에서 사용하는 언어가 자연어가 된다.<br>
자연어 처리(Natural Language Process)는 글을 쓰거나 대화를 주고받는 등 자연어를 사용하여 컴퓨터가 일을 처리하는 것을 언어처리라고 한다.<br>
최근 기계 번역의 정확도가 높아지거나 사람과 대화를 주고받는 등 기술력이 향상되고 있는 분야이기도 한다.</p>
<h2 id="활용-서비스">활용 서비스</h2>
<ol>
<li>문서검색 : 사용자의 검색 쿼리를 받아서 관련 문서를 찾아주는 서비스</li>
<li>기계번역 : 어떤 언어의 텍스트를 다른 언어로 변환하는 서비스</li>
<li>자동요약 : 어떤 문서의 내용을 중요한 내용만 선별하여 요약문을 생성해주는 서비스</li>
<li>자동상담 : 챗봇 혹은 음성으로 사용자의 문의사항을 자동으로 처리하는 서비스</li>
</ol>
<p>이외에도 인간의 대화방식이나 언어를 모방하여 일을 처리할 수 있는 많은 분야에서 활동되고 있다.</p>
<h2 id="글자레벨에서의-검색">글자레벨에서의 검색</h2>
<h3 id="기본적인-검색방법">기본적인 검색방법</h3>
<p>가장 단순하게 할 수 있는 검색방법은 문장의 왼쪽부터 한 글자씩 맞춰가는 방식이다. 보통 ctrl + F를 사용하는 방식과 유사하다고 생각할 수 있다.<br>
예를 들어 &quot;자연어처리는 무엇인가?&quot;에서 &quot;처리&quot;를 검색한다고 해보면 아래와 같이 된다.</p>
<table>
<thead>
<tr>
<th style="text-align:center">1</th>
<th style="text-align:center">2</th>
<th style="text-align:center">3</th>
<th style="text-align:center">4</th>
<th style="text-align:center">5</th>
<th style="text-align:center">6</th>
<th style="text-align:center">7</th>
<th style="text-align:center">8</th>
<th style="text-align:center">9</th>
<th style="text-align:center">10</th>
<th style="text-align:center">11</th>
<th style="text-align:center">12</th>
<th style="text-align:center">비고</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">자</td>
<td style="text-align:center">연</td>
<td style="text-align:center">어</td>
<td style="text-align:center">처</td>
<td style="text-align:center">리</td>
<td style="text-align:center">는</td>
<td style="text-align:center"></td>
<td style="text-align:center">무</td>
<td style="text-align:center">엇</td>
<td style="text-align:center">인</td>
<td style="text-align:center">가</td>
<td style="text-align:center">?</td>
<td style="text-align:center">원문</td>
</tr>
<tr>
<td style="text-align:center">처</td>
<td style="text-align:center">리</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">1번</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">처</td>
<td style="text-align:center">리</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">2번</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">처</td>
<td style="text-align:center">리</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">3번</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">처</td>
<td style="text-align:center">리</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">4번</td>
</tr>
</tbody>
</table>
<p>총 4번으로 해당하는 단어의 위치를 찾아낼 수 있다. 하지만 이 방법은 효율이 굉장히 낮다. 예시처럼 검색 대상 단어가 짧으면 상관없지만 길어지면 비교해야 하는 회수가 늘어나게 된다.</p>
<h3 id="Boyer-Moore-알고리즘">Boyer-Moore 알고리즘</h3>
<p>1977년에 R.S. Boyer와 J.S. Moore가 만든 알고리즘<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>으로 찾고자 하는 문자열의 마지막 자리가 맞는지 검사하면서 검색하는 방식으로 진행된다.</p>
<table>
<thead>
<tr>
<th style="text-align:center">글자</th>
<th style="text-align:center">이동글자수</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">리</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">처</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">그외</td>
<td style="text-align:center">2</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align:center">1</th>
<th style="text-align:center">2</th>
<th style="text-align:center">3</th>
<th style="text-align:center">4</th>
<th style="text-align:center">5</th>
<th style="text-align:center">6</th>
<th style="text-align:center">7</th>
<th style="text-align:center">8</th>
<th style="text-align:center">9</th>
<th style="text-align:center">10</th>
<th style="text-align:center">11</th>
<th style="text-align:center">12</th>
<th style="text-align:center">비고</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">자</td>
<td style="text-align:center">연</td>
<td style="text-align:center">어</td>
<td style="text-align:center">처</td>
<td style="text-align:center">리</td>
<td style="text-align:center">는</td>
<td style="text-align:center"></td>
<td style="text-align:center">무</td>
<td style="text-align:center">엇</td>
<td style="text-align:center">인</td>
<td style="text-align:center">가</td>
<td style="text-align:center">?</td>
<td style="text-align:center">원문</td>
</tr>
<tr>
<td style="text-align:center">처</td>
<td style="text-align:center">리</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">'연’이 처리에 없으므로 문자열 길이만큼 이동</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">처</td>
<td style="text-align:center">리</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">'처’가 '처리’에 있으므로 '리’를 찾을 수 있는 만큼 이동</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">처</td>
<td style="text-align:center">리</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">'처리’라는 단어를 찾음</td>
</tr>
</tbody>
</table>
<div class="video-container"><iframe src="https://www.youtube.com/embed/3Ft3HMizsCk" frameborder="0" loading="lazy" allowfullscreen></iframe></div>
<div class="video-container"><iframe src="https://www.youtube.com/embed/Tbj8iH9UkSA" frameborder="0" loading="lazy" allowfullscreen></iframe></div>
<h3 id="사전검색">사전검색</h3>
<p>검색 문자열이 사전의 어디에 위치하는지를 찾아내는 처리를 말하며 여기서 사전은 표제어 목록을 말한다.<br>
여기서 표제어란 &quot;책이나 장부 가운데 어떤 한 항목을 찾기 쉽게 설정한 말&quot;<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>로 대표어라고도 한다.</p>
<p>하지만 사전의 표제어는 어마어마한 양(위키피디아 한국어판은 표제어만 40만개가 넘고 영어는 500만개가 넘음)이므로 전체 표제어를 하나하나 비교하는 건 비효율적인 방법이다.</p>
<p>이런 과제를 해결하기 위해 고안한 방법이 이진검색(Binary Search)<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>와 트라이(Trie)<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>가 있다.</p>
<h4 id="이진검색">이진검색</h4>
<p>이진검색은 정렬된 배열에서 특정한 값의 위치를 찾는 알고리즘이다. 처음 중간 위치의 값을 선택하고 그 값의 크기를 비교하면서 찾아가게 된다.<br>
단, 검색시 정렬된 배열만 가능하다는 단점이 있지만 검색 프로세스를 반복할 때마다 값을 찾응ㄹ 수 있는 확률이 두 배가 되기 때문에 속도가 빠르다는 장점을 가지고 있다.</p>
<p>예를 들어 정렬된 배열에서 4를 찾는다고 한다면 아래와 같다.</p>
<table>
<thead>
<tr>
<th style="text-align:right">1</th>
<th style="text-align:right">2</th>
<th style="text-align:right">3</th>
<th style="text-align:right">4</th>
<th style="text-align:right">5</th>
<th style="text-align:right">60</th>
<th style="text-align:right">72</th>
<th style="text-align:right">83</th>
<th style="text-align:right">99</th>
<th style="text-align:right">101</th>
<th style="text-align:right">113</th>
<th style="text-align:center">비고</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right"></td>
<td style="text-align:right"></td>
<td style="text-align:right"></td>
<td style="text-align:right"></td>
<td style="text-align:right"></td>
<td style="text-align:right"></td>
<td style="text-align:right">4</td>
<td style="text-align:right"></td>
<td style="text-align:right"></td>
<td style="text-align:right"></td>
<td style="text-align:right"></td>
<td style="text-align:center">전체에서 중간인 72가 4보다 크므로 왼쪽으로 이동</td>
</tr>
<tr>
<td style="text-align:right"></td>
<td style="text-align:right"></td>
<td style="text-align:right">4</td>
<td style="text-align:right"></td>
<td style="text-align:right"></td>
<td style="text-align:right"></td>
<td style="text-align:right">X</td>
<td style="text-align:right">X</td>
<td style="text-align:right">X</td>
<td style="text-align:right">X</td>
<td style="text-align:right">X</td>
<td style="text-align:center">1~60의 중간인 3이 4보다 작으므로 오른쪽으로 이동</td>
</tr>
<tr>
<td style="text-align:right">X</td>
<td style="text-align:right">X</td>
<td style="text-align:right">X</td>
<td style="text-align:right"></td>
<td style="text-align:right">4</td>
<td style="text-align:right"></td>
<td style="text-align:right">X</td>
<td style="text-align:right">X</td>
<td style="text-align:right">X</td>
<td style="text-align:right">X</td>
<td style="text-align:right">X</td>
<td style="text-align:center">4~60의 중간인 5이 4보다 크므로 왼쪽으로 이동</td>
</tr>
<tr>
<td style="text-align:right">X</td>
<td style="text-align:right">X</td>
<td style="text-align:right">X</td>
<td style="text-align:right">4</td>
<td style="text-align:right">X</td>
<td style="text-align:right">X</td>
<td style="text-align:right">X</td>
<td style="text-align:right">X</td>
<td style="text-align:right">X</td>
<td style="text-align:right">X</td>
<td style="text-align:right">X</td>
<td style="text-align:center">값 찾음</td>
</tr>
</tbody>
</table>
<h4 id="트라이">트라이</h4>
<p>사전을 트리구조로 만드는 것과 같다고 생각하면 된다. 아래 그림처럼 표제어를 단어노드를 사용해서 연결하여 구성된다.</p>
<img src="data:image/svg+xml;base64,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" />
<p>이렇게 구성된 트리를 따라가면서 사전을 탐색하게 되는데 검색이 한번에 끝나기 때문에 매우 빠르게 진행됩니다.</p>
<h2 id="단어레벨의-검색">단어레벨의 검색</h2>
<p>컴퓨터가 문장을 이해하기 위해서 먼저 단어를 파악하고 단어의 품사를 판정해야 한다. 단어의 품사는 &quot;단어를 기능, 형태, 의미에 따라 나눈 갈래&quot;로 한국어에서는 명사, 대명사, 수사, 조사, 동사, 형용사, 관형사, 부사, 감탄사의 아홉가지로 분류된다.<br>
형태소분석방식은 규칙기반, 통계기반, 딥러닝기반으로 나누어 볼 수 있다.</p>
<h3 id="형태소-분석">형태소 분석</h3>
<p>형태소란 의미가 있는 최소의 단위로 문법적, 관계적인 뜻을 나타내는 단어 또는 단어의 부분을 말한다.<br>
단어의 품사를 판정하기 위해선 형태소 단위로 문장을 분리해야 하는데 이 과정을 형태소분석이라고 한다.</p>
<ul>
<li>자립성에 따른 형태소 분류</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:center">분류</th>
<th style="text-align:center"></th>
<th style="text-align:center"></th>
<th style="text-align:center"></th>
<th style="text-align:center"></th>
<th style="text-align:center"></th>
<th style="text-align:center">비고</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">자립형태소</td>
<td style="text-align:center">하늘</td>
<td style="text-align:center"></td>
<td style="text-align:center">매우</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">혼자쓰임</td>
</tr>
<tr>
<td style="text-align:center">의존형태소</td>
<td style="text-align:center"></td>
<td style="text-align:center">이</td>
<td style="text-align:center"></td>
<td style="text-align:center">푸르</td>
<td style="text-align:center">다</td>
<td style="text-align:center">붙어쓰임</td>
</tr>
</tbody>
</table>
<ul>
<li>의미에 따른 형태소 분류</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:center">분류</th>
<th style="text-align:center"></th>
<th style="text-align:center"></th>
<th style="text-align:center"></th>
<th style="text-align:center"></th>
<th style="text-align:center"></th>
<th style="text-align:center">비고</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">실질형태소</td>
<td style="text-align:center">하늘</td>
<td style="text-align:center"></td>
<td style="text-align:center">매우</td>
<td style="text-align:center">푸르</td>
<td style="text-align:center"></td>
<td style="text-align:center">실제 의미</td>
</tr>
<tr>
<td style="text-align:center">형식형태소</td>
<td style="text-align:center"></td>
<td style="text-align:center">이</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">다</td>
<td style="text-align:center">문법적 의미</td>
</tr>
</tbody>
</table>
<h4 id="형태소분석-방식">형태소분석 방식</h4>
<table>
<thead>
<tr>
<th style="text-align:center">분류</th>
<th style="text-align:center">내용</th>
<th style="text-align:center">단점</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">규칙기반</td>
<td style="text-align:center">자연어가 가지는 규칙과 특징을 사전으로 구축하여 패턴을 미리 정의해놓음</td>
<td style="text-align:center">사전 구축에 많은 리소스가 필요하며 모호한 규칙에 대해 대응하기 어려움</td>
</tr>
<tr>
<td style="text-align:center">통계기반</td>
<td style="text-align:center">말뭉치를 이용하여 대용량 텍스트에 나타나는 언어의 현상들을 일반화하는 통계를 활용하며 일반적인 자연어처리에 사용되고 있음</td>
<td style="text-align:center">사용하는 말뭉치에 따라 성능/품질이 달라지기 때문에 대용량 말뭉치가 필요함</td>
</tr>
</tbody>
</table>
<h4 id="관련-용어">관련 용어</h4>
<ol>
<li>말뭉치<br>
일상 대화를 기록해 둔 자료부터 신문기사, 소설 등 문자로 작성된 모든 것을 포괄하는 텍스트를 모아놓은 것으로 다음과 같은 특성을 가진다.</li>
</ol>
<ul>
<li>텍스트 수집이나 입력 과정에서 원래의 내용이나 형태의 누락이 있어서는 안되며, 원형을 유지하고 보장해야 한다.</li>
<li>언어의 특성을 잘 반영할 수 있는 구성으로 언어의 다양한 변이를 담아야 한다.</li>
<li>해당 언어의 통계적 대표성을 지녀야 하며 유의미한 규모로 확보되어야 한다.</li>
</ul>
<p>대표적으로 21세기 세종 계획의 결과로 구축된 말뭉치 중에는 현대국어 구어 전사 말뭉치, 한영/한일 병렬 말뭉치, 북한 및 해외 한국어 말뭉치 등이 포함되어 있다.<br>
말뭉치가 있으면 언어처리에 활용할 정확한 자료가 마련되지만 이러한 대규모 말뭉치를 구축하려면 상당히 많은 비용이 들어가게 되는 한계점을 가지고 있다.</p>
<ol start="2">
<li>Term<br>
정규화된 단어 혹은 글자(대문자, 형태, 철자 등)을 의미하며 상황에 따라선 단어와 동일한 레벨로 사용하기도 한다.</li>
</ol>
<blockquote>
<p>하늘 -&gt; 하,늘,하늘</p>
</blockquote>
<ol start="3">
<li>Token<br>
유용한 의미적 단위로 함께 모여지는 일련의 문자열로 구분 기호 사이의 글자 순서를 가진다.</li>
</ol>
<blockquote>
<p>하늘이 매우 푸르다 -&gt; 하늘이,매우,푸르다</p>
</blockquote>
<ol start="4">
<li>N-Gram<br>
N개의 문자열 크기 만큼 잘라서 문자열을 왼쪽에서 오른쪽으로 한 개씩 움직이며 추출되는 문자열의 집합</li>
</ol>
<blockquote>
<p>하늘이 매우 푸르다 -&gt; 2-gram -&gt; 하늘,늘이,이_,<em>매,매우,우</em>,_푸,푸르,르다</p>
</blockquote>
<ol start="5">
<li>조사</li>
<li>어미</li>
<li>선어말어미</li>
<li>어휘형태소</li>
<li>문법형태소</li>
<li>용언</li>
<li>제언</li>
<li>접미사</li>
<li>전성어미</li>
<li>어근부</li>
<li>용언화 접미사</li>
<li>서술격 조사</li>
</ol>
<h4 id="일반적인-형태소분석">일반적인 형태소분석</h4>
<p>형태소 분석의 일반적인 순서는 아래와 같다.</p>
<img src="data:image/svg+xml;base64,<svg width="811.5" height="696" version="1.1" baseProfile="full" viewbox="0 0 811.5 696" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:ev="http://www.w3.org/2001/xml-events" style="font-weight:bold; font-size:12pt; font-family:'Calibri', Helvetica, sans-serif;;stroke-width:3;stroke-linejoin:round;stroke-linecap:round"><path d="M508.8 44.5 L508.8 64.5 L508.8 84.5 L508.8 84.5 " style="stroke:#33322E;fill:none;stroke-dasharray:none;"></path>
<path d="M503.4 71.2 L508.8 77.8 L514.1 71.2 L508.8 84.5 Z" style="stroke:#33322E;fill:#33322E;stroke-dasharray:none;"></path>
<path d="M508.8 115.5 L508.8 135.5 L508.8 155.5 L508.8 155.5 " style="stroke:#33322E;fill:none;stroke-dasharray:none;"></path>
<path d="M503.4 142.2 L508.8 148.8 L514.1 142.2 L508.8 155.5 Z" style="stroke:#33322E;fill:#33322E;stroke-dasharray:none;"></path>
<path d="M490.2 186.5 L466.3 206.5 L466.3 226.5 L466.3 226.5 " style="stroke:#33322E;fill:none;stroke-dasharray:none;"></path>
<path d="M460.9 213.2 L466.3 219.8 L471.6 213.2 L466.3 226.5 Z" style="stroke:#33322E;fill:#33322E;stroke-dasharray:none;"></path>
<path d="M440.1 257.5 L406.3 277.5 L406.3 297.5 L406.3 297.5 " style="stroke:#33322E;fill:none;stroke-dasharray:none;"></path>
<path d="M400.9 284.2 L406.3 290.8 L411.6 284.2 L406.3 297.5 Z" style="stroke:#33322E;fill:#33322E;stroke-dasharray:none;"></path>
<path d="M381.5 328.5 L349.5 348.5 L349.5 368.5 L349.5 368.5 " style="stroke:#33322E;fill:none;stroke-dasharray:none;"></path>
<path d="M344.2 355.2 L349.5 361.8 L354.8 355.2 L349.5 368.5 Z" style="stroke:#33322E;fill:#33322E;stroke-dasharray:none;"></path>
<path d="M325.4 399.5 L294.3 419.5 L294.3 439.5 L294.3 439.5 " style="stroke:#33322E;fill:none;stroke-dasharray:none;"></path>
<path d="M288.9 426.2 L294.3 432.8 L299.6 426.2 L294.3 439.5 Z" style="stroke:#33322E;fill:#33322E;stroke-dasharray:none;"></path>
<path d="M255.8 470.5 L206.3 490.5 L206.3 510.5 L206.3 510.5 " style="stroke:#33322E;fill:none;stroke-dasharray:none;"></path>
<path d="M200.9 497.2 L206.3 503.8 L211.6 497.2 L206.3 510.5 Z" style="stroke:#33322E;fill:#33322E;stroke-dasharray:none;"></path>
<path d="M171.3 541.5 L126.3 561.5 L126.3 581.5 L126.3 581.5 " style="stroke:#33322E;fill:none;stroke-dasharray:none;"></path>
<path d="M120.9 568.2 L126.3 574.8 L131.6 568.2 L126.3 581.5 Z" style="stroke:#33322E;fill:#33322E;stroke-dasharray:none;"></path>
<path d="M87.5 612.5 L37.5 632.5 L37.5 652.5 L37.5 652.5 " style="stroke:#33322E;fill:none;stroke-dasharray:none;"></path>
<path d="M32.2 639.2 L37.5 645.8 L42.8 639.2 L37.5 652.5 Z" style="stroke:#33322E;fill:#33322E;stroke-dasharray:none;"></path>
<path d="M582.3 181.4 L759 206.5 L759 419.5 L759 419.5 L714.4 439.5 L714.4 439.5 " style="stroke:#33322E;fill:none;stroke-dasharray:none;"></path>
<path d="M511.3 248.9 L696.5 277.5 L696.5 419.5 L696.5 419.5 L687.1 439.5 L687.1 439.5 " style="stroke:#33322E;fill:none;stroke-dasharray:none;"></path>
<path d="M421.3 245.7 L31.8 277.5 L31.8 419.5 L31.8 419.5 L67 439.5 L67 439.5 " style="stroke:#33322E;fill:none;stroke-dasharray:none;"></path>
<path d="M511.3 246.8 L799 277.5 L799 490.5 L799 490.5 L633.3 515.1 L633.3 515.1 " style="stroke:#33322E;fill:none;stroke-dasharray:none;"></path>
<path d="M451.3 320 L634 348.5 L634 419.5 L634 419.5 L659.8 439.5 L659.8 439.5 " style="stroke:#33322E;fill:none;stroke-dasharray:none;"></path>
<path d="M361.3 318.1 L94.3 348.5 L94.3 419.5 L94.3 419.5 L94.3 439.5 L94.3 439.5 " style="stroke:#33322E;fill:none;stroke-dasharray:none;"></path>
<path d="M451.3 327.4 L517.3 348.5 L517.3 490.5 L517.3 490.5 L541.2 510.5 L541.2 510.5 " style="stroke:#33322E;fill:none;stroke-dasharray:none;"></path>
<path d="M423 394.7 L594 419.5 L642.3 439.5 L642.3 439.5 " style="stroke:#33322E;fill:none;stroke-dasharray:none;"></path>
<path d="M293.2 399.5 L220.5 419.5 L149.4 439.5 L149.4 439.5 " style="stroke:#33322E;fill:none;stroke-dasharray:none;"></path>
<path d="M389.2 399.5 L440.5 419.5 L440.5 490.5 L440.5 490.5 L507.7 510.5 L507.7 510.5 " style="stroke:#33322E;fill:none;stroke-dasharray:none;"></path>
<path d="M334.4 470.5 L386.3 490.5 L486.3 511 L486.3 511 " style="stroke:#33322E;fill:none;stroke-dasharray:none;"></path>
<path d="M290.8 470.5 L286.3 490.5 L286.3 632.5 L286.3 632.5 L241.2 652.5 L241.2 652.5 " style="stroke:#33322E;fill:none;stroke-dasharray:none;"></path>
<path d="M206.3 541.5 L206.3 561.5 L206.3 632.5 L206.3 632.5 L206.3 652.5 L206.3 652.5 " style="stroke:#33322E;fill:none;stroke-dasharray:none;"></path>
<path d="M246.3 533 L409.5 561.5 L409.5 632.5 L409.5 632.5 L398.2 652.5 L398.2 652.5 " style="stroke:#33322E;fill:none;stroke-dasharray:none;"></path>
<path d="M126.3 612.5 L126.3 632.5 L171.3 652.5 L171.3 652.5 " style="stroke:#33322E;fill:none;stroke-dasharray:none;"></path>
<path d="M166.3 602.8 L369.5 632.5 L380.8 652.5 L380.8 652.5 " style="stroke:#33322E;fill:none;stroke-dasharray:none;"></path>
<rect x="484.5" y="13.5" height="31" width="48" style="stroke:#33322E;fill:#ffffff;stroke-dasharray:none;"></rect>
<text x="492.5" y="35" style="">입력</text>
<rect x="476.5" y="84.5" height="31" width="64" style="stroke:#33322E;fill:#ffffff;stroke-dasharray:none;"></rect>
<text x="484.5" y="106" style="">전처리</text>
<rect x="435.5" y="155.5" height="31" width="147" style="stroke:#33322E;fill:#ffffff;stroke-dasharray:none;"></rect>
<text x="443.5" y="177" style="">문법 형태소 분리</text>
<rect x="421.5" y="226.5" height="31" width="90" style="stroke:#33322E;fill:#ffffff;stroke-dasharray:none;"></rect>
<text x="429.8" y="248" style="">체언 분석</text>
<rect x="361.5" y="297.5" height="31" width="90" style="stroke:#33322E;fill:#ffffff;stroke-dasharray:none;"></rect>
<text x="369.8" y="319" style="">용언 분석</text>
<rect x="276.5" y="368.5" height="31" width="147" style="stroke:#33322E;fill:#ffffff;stroke-dasharray:none;"></rect>
<text x="284.5" y="390" style="">단일 형태소 분석</text>
<rect x="207.5" y="439.5" height="31" width="173" style="stroke:#33322E;fill:#ffffff;stroke-dasharray:none;"></rect>
<text x="215.8" y="461" style="">복합명사 / 미등록어</text>
<rect x="166.5" y="510.5" height="31" width="80" style="stroke:#33322E;fill:#ffffff;stroke-dasharray:none;"></rect>
<text x="174.5" y="532" style="">준말처리</text>
<rect x="86.5" y="581.5" height="31" width="80" style="stroke:#33322E;fill:#ffffff;stroke-dasharray:none;"></rect>
<text x="94.5" y="603" style="">조사생략</text>
<rect x="13.5" y="652.5" height="31" width="48" style="stroke:#33322E;fill:#ffffff;stroke-dasharray:none;"></rect>
<text x="21.5" y="674" style="">결과</text>
<rect x="606.5" y="439.5" height="31" width="147" style="stroke:#33322E;fill:#ffffff;stroke-dasharray:none;"></rect>
<text x="614.5" y="461" style="">문법 형태소 사전</text>
<rect x="20.5" y="439.5" height="31" width="147" style="stroke:#33322E;fill:#ffffff;stroke-dasharray:none;"></rect>
<text x="28.5" y="461" style="">어휘 형태소 사전</text>
<rect x="486.5" y="510.5" height="31" width="147" style="stroke:#33322E;fill:#ffffff;stroke-dasharray:none;"></rect>
<text x="494.5" y="532" style="">분야별 용어 사전</text>
<rect x="132.5" y="652.5" height="31" width="147" style="stroke:#33322E;fill:#ffffff;stroke-dasharray:none;"></rect>
<text x="140.5" y="674" style="">사용자 정의 사전</text>
<rect x="336.5" y="652.5" height="31" width="106" style="stroke:#33322E;fill:#ffffff;stroke-dasharray:none;"></rect>
<text x="344.8" y="674" style="">기분석 사전</text></svg>" />
<h5 id="전처리">전처리</h5>
<p>입력 문자열로부터 문장 부호, 숫자, 영문자와 같이 형태소 분석 대상이 되지 않는 문자들을 분리하거나 제거하게 된다.<br>
일반적으로 단어를 추출할 때 문장부호는 별개의 단어로 독립시키고 불필요한 문자(제어문자,특수문자 등)는 분석이 되지 않도록 공백으로 대치한다.</p>
<blockquote>
<p>나는~~ 학교 간다 ^^ -&gt; 나는  학교 간다</p>
</blockquote>
<h5 id="문법-형태소-분리">문법 형태소 분리</h5>
<p>단어를 구성하고 있는 각 형태소들을 분리하는 단계로 한국어의 단어형성규칙에 따라 형태소인 조사와 어미를 분리하게 된다. 어미가 있는 경우 선어말어미를 분리하고 어휘형태소와 문법형태소간의 결합상태를 검사하게 된다.<br>
어미가 분리되어 용언으로 추정되는 단어에 대해서 불규칙 활용이 일어났을 가능성이 있는 경우 불규칙 원형 복원에 의해 원형을 추정하게 된다.</p>
<blockquote>
<p>나는 학교 간다 -&gt; 나/는 /학교/ /간/다</p>
</blockquote>
<h5 id="체언분석">체언분석</h5>
<p>형태소 분리 결과 조사가 분리된 단어는 체언으로 추정한다. 어근부가 체언이 아닌 경우 접미사를 분리하고 체언 분리를 시도하게 된다. 명사형 전성어미가 발견된 단어는 용언 분석 단계에서 처리하게 된다.</p>
<blockquote>
<p>나/는 /학교/ /간/다 -&gt; 나/대명사 는/조사</p>
</blockquote>
<h5 id="용언분석">용언분석</h5>
<p>형태소 분리 결과 어미가 분리된 단어는 용언으로 추정하게 되며 어휘형태소 사전에서 용언 분석을 시도한다. 어근부가 용언이 아닌 경우에는 용언화 접미사를 분리하여 '-하다/되다/시키다’와 같은 접미사가 발견되면 체언분석을 시도하게 된다. 서술격 조사 '이’가 발견되거나 서술격 조사가 생략된 경우에도 체언분석을 시도한다. 본 용언과 보조 용언이 결합된 유형이 있는 경우 보조 용언을 먼저 분석하고 본 용언을 분석하게 된다.</p>
<blockquote>
<p>나/는 /학교/ /간/다 -&gt; 날/동사 + 는/어미, 가/동사+ㄴ다/어미, 갈/동사+ㄴ다/어미</p>
</blockquote>
<h5 id="단일-형태소-분석">단일 형태소 분석</h5>
<p>형태소 분리 단계에서 조사나 어미가 발견되지 않은 단어는 단일 형태소로 추정하며 어휘형태소를 사전에서 체언, 부사어, 감탄어, 관형어인지 확인한다. 접미사가 분리되는 경우 접미사를 분리하고 어휘형태소 사전을 확인하게 된다.</p>
<blockquote>
<p>나/는 /학교/ /간/다 -&gt; 학교/일반명사</p>
</blockquote>
<h5 id="복합명사-및-미등록어-추정">복합명사 및 미등록어 추정</h5>
<p>조사가 분리되어 체언이 추정된 어근부가 체언으로 분석되지 않는 단어는 어근부가 복합명사로 인식될 수 있는지 확인한다.<br>
또한 분석단계에서 분석되지 않은 단어는 단어형성 규칙을 바탕으로 미등록어를 추정하게 된다.</p>
<h5 id="준말처리">준말처리</h5>
<p>준말에 대해서는 본딧말 규칙이나 기분석 사전을 통해 처리하게 된다.</p>
<blockquote>
<p>SSD -&gt; Solid State Drive</p>
</blockquote>
<h5 id="후처리">후처리</h5>
<p>영문자나 숫자 등 한글 이외의 문자열에 대해서 출력이 필요하면 연결하여 출력하게 되며 모호성 해결과 같은 형태소 분석의 정확도를 높이기 위한 작업을 진행한다.</p>
<h4 id="한국어-형태소분석의-어려움">한국어 형태소분석의 어려움<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup></h4>
<ol>
<li>용언의 불규칙 활용, 축약, 모음탈락 등 형태적 변형요인이 다양하다.</li>
</ol>
<blockquote>
<p>나는 집에 간다. -&gt; 나 집에 간다.</p>
</blockquote>
<ol start="2">
<li>어휘형태소와 문법형태소의 결합과정에서 형태적 변형이 발생한다.</li>
<li>형태소만 가지고는 정의가 안되는 것들이 많다.</li>
</ol>
<blockquote>
<p>비가 온다 -&gt; 가수 비? 하늘에서 내리는 비?</p>
</blockquote>
<ol start="4">
<li>조사나 어미는 음소단위로 결합되기 때문에 형태 분석을 위해서는 음절단위의 글자를 음소단위의 자소로 나누어야 한다.</li>
</ol>
<blockquote>
<p>간다 -&gt; 가+ㄴ다.</p>
</blockquote>
<h4 id="통계기반-형태소분석">통계기반 형태소분석</h4>
<p>예를 들어 &quot;검은콩&quot;를 분석하게 되면 &quot;검은/형용사 콩/명사&quot;와 &quot;검은콩/명사&quot;라는 후보 중 많이 나온 형태가 &quot;검은콩/명사&quot;라면 후보 선택으로 &quot;검은콩/명사&quot;를 출력하게 되는 방식을 말한다.<br>
통계적인 형태소분석을 사용하기 위해서는 말뭉치(corpus)가 필요하게 되는데 예를 들면 다음과 같다.</p>
<blockquote>
<p>[검정/명사 고무신/명사]<br>
[2019/숫자 년/명사]<br>
[임용고시/명사]</p>
</blockquote>
<p>이렇게 통계를 사용하다보면 제로 빈도 문제<sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup>가 발생하게 되는데 이는 긴 문장이나 보기 드문 단어가 들어간 단어 열에 대한 말뭉치 내의 빈도가 0이어서 확률을 제대로 계산하지 못하는 문제를 말한다. 이러한 경우는 품사를 제대로 판정하기 어렵고 이에 대한 처리가 반드시 필요하게 된다.</p>
<h4 id="딥러닝기반-형태소분석">딥러닝기반 형태소분석</h4>
<p>카카오형태소분석기 Khaiii <a href="https://github.com/kakao/khaiii">https://github.com/kakao/khaiii</a></p>
<h3 id="임베딩-Embedding">임베딩(Embedding)</h3>
<p>컴퓨터는 자연어를 사람처럼 이해할 수 없으며 임베딩을 활용하여 자연어를 계산하는 것이 가능하다. 임베딩은 자연어를 컴퓨터가 처리할 수 있는 숫자들의 나열인 벡터로 바꾼 결과이다.<br>
화자가 얘기하는 것 중 중요한 부분은 세가지로 나눠볼 수 있다고 가정한다.</p>
<ol>
<li>문장에 어떤 단어가 많이 쓰였는가?</li>
<li>단어가 어떤 순서로 등장하는가?</li>
<li>문장에 어떤 단어가 같이 나타났는가?</li>
</ol>
<h3 id="백오브워즈-Bag-of-Words">백오브워즈(Bag of Words)</h3>
<p>백오브워즈에서는 어떤 단어가 많이 사용되었는지가 중요하고 단어의 순서는 무시하게 된다.<br>
백워브워즈는 문장을 형태소분석을 통해 단어들로 나누고 모든 단어의 유무를 나타낼 수 있는 배열에 넣어 빈도를 나타내게 된다.</p>
<blockquote>
<p>소비가 줄고 경기가 얼어붙자, 일자리도 빠르게 사라졌습니다.<br>
3월의 고용 통계를 보면 지난해 같은 달에 비해서 사업체에서 일하는 사람의 숫자가 줄었습니다.<br>
지난 3월 국내 사업자 종자사 수는 지난해 같은 달과 비교해 22만5000명 줄었습니다.<br>
2009년 관련 통계를 처음 낸 이후 11년 만에 첫 마이너스입니다.<br>
상대적으로 고용구조가 취약한 일자리가 큰 타격을 받았습니다.</p>
</blockquote>
<table>
<thead>
<tr>
<th style="text-align:center">국내</th>
<th style="text-align:center">달</th>
<th style="text-align:center">비교</th>
<th style="text-align:center">통계</th>
<th style="text-align:center">처음</th>
<th style="text-align:center">마이너스</th>
<th style="text-align:center">일자리</th>
<th style="text-align:center">…</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">2</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">2</td>
<td style="text-align:center">…</td>
</tr>
</tbody>
</table>
<p>백오브워즈에서는 단어의 빈도가 비슷하면 관련성이 상대적으로 높을 것이라 추정하며 정보검색에서는 사용자의 질의에 가장 적절한 문서를 보여줄 때 질의를 백오브워즈로 변환하고 대상 문서에서 빈도 통계상 유사도가 높은 문서를 찾아주게 된다.</p>
<h3 id="TF-IDF-Term-Frequency-Inverse-Document-Frequency">TF-IDF(Term Frequency-Inverse Document Frequency)</h3>
<p>예를 들어 달/월 등 어떤 문서에서 단어가 많이 나온 것만 가지고 유사하다고 보기 어려운 경우가 많기 때문에 이러한 단점을 극복하기 위해 제안되었다.</p>
<p>$$ TF-IDF(w) = TF(w) \times log( \frac{N}{DF(w)}) $$</p>
<p>수식에서 \(TF\)는 특정 문서에서 특정단어가 나타난 수를 나타내고 \(DF\)는 대상 문서 모두에서 특정 단어가 나타는 수를 나타낸다. 여기서 수식에 의해 \(IDF\)는 값이 클수록 특이한 단어라는 걸 나타내게 된다.<br>
즉, 어떤 단어의 특이성이 높아질수록 가중치가 높아지게 되어 문서의 주제를 예측하기 쉬워진다.</p>
<div class="video-container"><iframe src="https://www.youtube.com/embed/ngYeCV57GlQ" frameborder="0" loading="lazy" allowfullscreen></iframe></div>
<h3 id="Deep-Averaging-Network">Deep Averaging Network</h3>
<p>백워브워즈의 머신러닝 기법으로 백워브워즈처럼 단어를 순서를 고려하지 않고 어떤 단어가 얼마나 많이 사용하고 있는지에 따라 문서를 분류한다. 간단한 구조의 아키텍쳐임에도 성능이 좋아서 많이 사용되고 있다.</p>
<img src="data:image/svg+xml;base64,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" />
<p>$$ V = \frac {1}{n} \times \sum_{i=1}^{n} e^i $$</p>
<h3 id="NPLM-Neural-Probabilistic-Language-Model">NPLM(Neural Probabilistic Language Model)</h3>
<p>요슈아 벤지오 연구팀이 2003년도에 제안한 방법으로 학습 데이터에 한 번도 등장하지 않는 단어(혹은 N-Gram)가 등장했을 때 문제가 발생할 수 있는 등의 전통적인 언어 모델의 단점을 극복하고자 했다.<br>
당시에는 NPLM으로 발표 했으며 현재는 Neural Network Language Model(Feed Forward Neural Network Language Model,NNLM)로도 불린다.<br>
베지오 연구팀은 기존 언어 모델의 단점을 다음과 같이 정의하였다.</p>
<ol>
<li>학습 데이터에 존재하지 않는 단어(n-gram)이 포함된 단어가 나타날 수 있다.</li>
<li>과거의 정보가 마지막까지 전달되지 않는 장기 의존성(long-term dependency)을 포착하기 어렵다.</li>
<li>단어/문장 간 유사도를 계산할 수 없다.</li>
</ol>
<p>NPLM은 직전까지 등장한 n-1 개 단어들로 다음 단어를 맞추는 n-gram 언어모델이며 수식은 다음과 같다.</p>
<p>$$ P(w_t|w_{t-1},…,w_{t-n+1}) = \frac{exp(Y_{w_t})}{\sum_i exp(Y_i)} \\<br>
w_t : t번째 단어 , Y_{w_t} : 출력 $$</p>
<img src="data:image/svg+xml;base64,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" />
<p>NPLM은 Input Layer(입력층), Projection Layer(투사층), Hidden Layer(은닉층), Output Layer(출력층)으로 이루어져 있다.</p>
<h4 id="입력단계">입력단계</h4>
<p>입력단계는 입력층에서 투사층까지의 처리를 하게 된다.<br>
$$ X_t = C \cdot w_t = C(w_t)$$<br>
입력은 문장 내 t번째 단어(\(w_t\))에 대응하는 단어 벡터 \(X_t\)를 만드는 과정으로 행렬 \(C\)와 \(w_t\)에 해당하는 원핫벡터<sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup>를 한 것과 같다. \(C\)라는 행렬에서 \(w_t\)에 해당하는 행만 가져오게 되며 이렇게 만들어진 테이블을 참조(lookup)테이블이라고 한다.</p>
<p>예를 들어 \( \begin{bmatrix} 1&amp;2&amp;3 \cr 4&amp;5&amp;6 \cr 7&amp;8&amp;9 \end{bmatrix} \) 배열에 대해 3번째 값만 참조한다면 다음과 같이 표현할 수 있다.</p>
<p>$$ C(w_3) = \begin{bmatrix} 0&amp;0&amp;1 \end{bmatrix} \times \begin{bmatrix} 1&amp;2&amp;3 \cr 4&amp;5&amp;6 \cr 7&amp;8&amp;9 \end{bmatrix} = \begin{bmatrix} 7&amp;8&amp;9 \end{bmatrix} $$</p>
<p>예를 들어 “여름 아이스크림 벤치 시원 기분”(여름에 아이스크림을 벤치에서 먹으면 시원하고 기분이 좋다) 에서 &quot;아이스크림&quot;를 예측한다고 한다면 다음과 같이 진행된다.<br>
<em>단어레벨에서 진행되기 때문에 조사 등은 생략하고 단어로만 예시를 들었음</em></p>
<p>원핫벡터를 생성하게 되면 아래와 같이 된다.<br>
$$여름 = \begin{bmatrix} 1&amp;0&amp;0&amp;0&amp;0 \end{bmatrix} \\<br>
아이스크림 = \begin{bmatrix} 0&amp;1&amp;0&amp;0&amp;0 \end{bmatrix} \\<br>
시원 = \begin{bmatrix} 0&amp;0&amp;1&amp;0&amp;0 \end{bmatrix} \\<br>
벤치 = \begin{bmatrix} 0&amp;0&amp;0&amp;1&amp;0 \end{bmatrix} \\<br>
기분 = \begin{bmatrix} 0&amp;0&amp;0&amp;0&amp;1 \end{bmatrix}$$</p>
<p>여기서는 가중치 벡터 \(w_t\)는 투사층의 크기(m)를 3으로 설정하게 되면 아래와 같다고 가정하자.<br>
$$ w_t = \begin{bmatrix} 0.1&amp;0.2&amp;0.3 \cr 0.4&amp;0.5&amp;0.6 \cr 0.7&amp;0.8&amp;0.9 \cr 1.0&amp;1.1&amp;1.2 \cr 1.3&amp;1.4&amp;1.5 \end{bmatrix} $$</p>
<p>여기서 &quot;시원&quot;일 경우 원핫벡터를 생성하면 아래와 같이 계산된다.<br>
$$ C(w_{시원}) = \begin{bmatrix} 0&amp;1&amp;0&amp;0 \end{bmatrix} \cdot \begin{bmatrix} 0.1&amp;0.2&amp;0.3 \cr 0.4&amp;0.5&amp;0.6 \cr 0.7&amp;0.8&amp;0.9 \cr 1.0&amp;1.1&amp;1.2 \cr 1.3&amp;1.4&amp;1.5 \end{bmatrix} = \begin{bmatrix} 0.4&amp;0.5&amp;0.6 \end{bmatrix} $$</p>
<h4 id="출력단계">출력단계</h4>
<p>출력단계는 투사층에서 은닉층을 거쳐 출력층으로 이루어지게 된다.<br>
NPLM 수식에서 \(Y\)는 말뭉치 전체의 단어수(V)에 해당하는 차원을 가진 스코어 벡터이다.<br>
예를 들어 전체 단어수가 3이면 다음과 같이 나타낼 수 있다.<br>
$$ Y_{w_t} = \begin{bmatrix} 1 \cr 2 \cr 3 \cr \end{bmatrix} , V=3 $$</p>
<p>결국 출력은 V차원의 스코어 벡터에 softmax함수를 적용한 V차원의 확률벡터이다.<br>
NPLM은 확률값이 가장 높은 단어의 인덱스의 단어가 실제 정답 단어와 일치하도록 학습이 진행된다.<br>
$$ Y_{w_t} = b + U \cdot \tanh (d+H_{x_t}) \\<br>
b,d : bias벡터\\<br>
H : 입력층에서 은닉층으로 보내주는 가중치 매트릭스\\<br>
U : 은닉층에서 출력층으로 보내주는 가중치 매트릭스 $$</p>
<p>앞의 예를 다시 가져오게 되면 아래와 같이 된다.<br>
$$ Y_{w_t} = \begin{bmatrix} 0.12 \cr 0.7 \cr 0.23 \cr 0.3 \cr 0.13 \end{bmatrix} = \begin{bmatrix} 0 \cr 1 \cr 0 \cr 0 \cr 0 \end{bmatrix} = \begin{bmatrix} 여름 \cr 아이스크림 \cr 벤치 \cr 시원 \cr 기분 \end{bmatrix} = 아이스크림 $$</p>
<p>이런 방식을 통해 단어들의 순서에 의해 이전 단어를 통해 단어를 예측할 수 있게 된다.</p>
<h3 id="Word2Vec">Word2Vec</h3>
<p>2013년 구글 연구팀이 &quot;Efficient Estimation of Word Representations in Vector Space(Mikolov et al., 2013a)<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup>&quot;과  &quot;Distributed Representation of Words and Phrase and their Compositionality(Mikolov et al., 2013b)<sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup>&quot;를 발표하여 가장 널리 쓰이고 있는 단어레벨의 임베딩 모델이다.<br>
&quot;2013a&quot;에서는 Skip-Gram과 CROW(Continuous Bag of Words)가 제안되었고 &quot;2013b&quot;에서는 두 모델을 기반으로 하여 네거티브 샘플링<sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup> 등 학습 최적화 기법을 제안한 내용이 핵심 골자이다.</p>
<ul>
<li>CROW</li>
</ul>
<img src="data:image/svg+xml;base64,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" />
<ul>
<li>Skip-gram</li>
</ul>
<img src="data:image/svg+xml;base64,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" />
<p>CROW는 주변에 있는 단어(문맥단어, context word)들을 가지고 알고자하는 단어(중심단어, 타깃단어, center word, target word)를 맞추는 과정으로 학습하고 Skip-gram은 중심단어를 가지고 주변의 문맥단어를 무엇일지 예측하는 과정으로 학습하게 된다.<br>
단순히 학습에 사용되는 데이터쌍으로 본다면 CROW는 4:1(w1~4:W)인 반면 Skip-gram은 1:1 * 4(W:w1, W:w2, W:w3, W:w4)로 더 많은 학습데이터쌍을 확보할 수 있어 일반적으로 품질이 좋은 편이다.</p>
<h4 id="CROW">CROW</h4>
<p>“여름 아이스크림 벤치 시원 기분”(여름에 아이스크림을 벤치에서 먹으면 시원하고 기분이 좋다) 에서 &quot;아이스크림&quot;를 예측한다고 한다고 하고 정답을 \(y\)라고 설정하고 학습과정을 시작해보자.</p>
<ol>
<li>전체 크기가 \(m\) 인 입력 데이터를 원핫벡터 (\(x^{(c-m)} ,…, x^{(c-1)} , x^{(c+1)} ,…, x^{(c+m)}\)) 로 변경하면 아래와 같이 된다. 여기서 \(c\)는 중심단어의 위치를 나타낸다.</li>
</ol>
<p>$$x^{여름,c-1} = \begin{bmatrix} 1&amp;0&amp;0&amp;0 \end{bmatrix} \\<br>
x^{시원,c+1} = \begin{bmatrix} 0&amp;1&amp;0&amp;0 \end{bmatrix} \\<br>
x^{벤치,c+2} = \begin{bmatrix} 0&amp;0&amp;1&amp;0 \end{bmatrix} \\<br>
x^{기분,c+3} = \begin{bmatrix} 0&amp;0&amp;0&amp;1 \end{bmatrix}$$</p>
<ol start="2">
<li>
<p>\(V\)는 입력워드행렬(input word matrix)로 \(V\)의 \(i\)번째 열이 임베디드 벡터인 \(x^i\)를 입력되게 되며 입력레이어의 데이터가 된다.<br>
$$ v_{c−m}=Vx^{(c−m)} , v_{c−m+1}=Vx^{(c−m+1)} ,…, v_{c+m}=Vx^{(c+m)}$$</p>
</li>
<li>
<p>입력워드행렬을에 대한 평균을 내게 된다.<br>
$$ \hat v = \frac{(v_{c−m}=Vx^{(c−m)} , v_{c−m+1}=Vx^{(c−m+1)} ,…, v_{c+m}=Vx^{(c+m)})}{2m} $$</p>
</li>
<li>
<p>\(U\)는 출력워드행렬(output word matrix)로 \(U\)의 \(j\)번째 열이 n차원의 임베디드 벡터로 표현된다. 이를 활용하여 스코어를 구하면 아래와 같이 된다.<br>
$$ z = U \hat v $$</p>
</li>
<li>
<p>스코어를 확률로 변경한다.<br>
$$ \hat y = softmax(z) $$</p>
</li>
</ol>
<p>\( \hat y \)가 정답인 \(y\)와 같아지도록 반복 학습시켜야 한다.<br>
\(U\)의 \(j\)번째 열을 \(U_j\)로 표기할 때, \( z_j = U_j \cdot \hat{v}\)가 크면 클수록,즉 \(U_j\)와 \(\hat{v}\)가 비슷한 벡터일수록 \(x^j\)가 중심단어로 채택될 확률이 높아진다.</p>
<h4 id="Skip-gram">Skip-gram</h4>
<p>포지티브 샘플은 중심 단어 주변에 등장한 단어를 말하며 네거티브 샘플은 주변에 등장하지 않은 단어의 쌍을 말한다.<br>
CROW의 예제를 다시 활용하고 윈도우<sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup>를 2로 설정한다면:</p>
<p>\( t: target word , c : context word \)</p>
<p>포지티브 샘플은 아래와 같다.</p>
<table>
<thead>
<tr>
<th style="text-align:center">t</th>
<th style="text-align:center">c</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">아이스크림</td>
<td style="text-align:center">여름</td>
</tr>
<tr>
<td style="text-align:center">아이스크림</td>
<td style="text-align:center">시원</td>
</tr>
</tbody>
</table>
<p>네거티브 샘플은 다음과 같다.</p>
<table>
<thead>
<tr>
<th style="text-align:center">t</th>
<th style="text-align:center">c</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">아이스크림</td>
<td style="text-align:center">전자렌지</td>
</tr>
<tr>
<td style="text-align:center">아이스크림</td>
<td style="text-align:center">가을</td>
</tr>
<tr>
<td style="text-align:center">아이스크림</td>
<td style="text-align:center">분노</td>
</tr>
</tbody>
</table>
<p>Skip-gram모델은 CROW와 비슷하긴 하지만 다음과 같이 진행되게 된다.</p>
<ol>
<li>원핫벡터로 입력워드행렬 \(x\)를 만든다.</li>
<li>임베이드 벡터인 \(vc = Vx\)를 만든다.</li>
<li>평균이 아니라 \( \hat v = vc \)를 사용한다.</li>
<li>\(u = Uv_c\)를 사용해서 스코어 벡터(\(u_{c−m} , u_{c−m+1} ,…, u_{c+m})\)를 만든다.</li>
<li>각 스코어를 확률로 변환한다.<br>
$$ y = softmax(u) $$</li>
</ol>
<p>CROW에서도 사용할 수 있지만 모델을 평가하기 위한 손실함수(loss function, objective function)은 아래와 같이 사용할 수 있다.</p>
<p>$$ minimize J<br>
= - \log P(w_{c-m},…,w_{c-1},w_{c+1},…w_{c+m}|w_c) \\<br>
= - \log \prod_{j=0, j \neq m}^{2m} P(w_{c-m+j}|w_c) \\<br>
= - \log \prod_{j=0, j \neq m}^{2m} P(u_{c-m+j}|v_c) \\<br>
= - \log \prod_{j=0, j \neq m}^{2m} \frac{\exp(u_{c-m+j}^T)v_c}{ \sum_{k=1}^{|W|} \exp(u_k^Tv_c)} \\<br>
= - \sum_{j=0, j \neq m}^{2m} u_{c-m+j}^T + 2m \log \sum_{k=1}^{|W|} \exp(u_k^Tv_c) \\<br>
$$</p>
<p>각 단계마다 unknown 파라미터들에 대한 기울기를 계산할 수 있고, 각 단계마다 SGD(Stochastic Gradient Descent)를 이용하여 파라미터들을 업데이트 할 수 있다. 수식으로 표현하면,<br>
$$<br>
J<br>
= - \sum_{j=0, j \neq m}^{2m} \log P(u_{c-m+j}|v_c) \\<br>
= \sum_{j=0, j \neq m}^{2m} H(\hat y, y_{c-m+j})<br>
$$</p>
<p>\(H(\hat{y},y_{c-m+j}\))는 probability vector \(\hat{y}\)와 one-hot vector \(y_{c-m+j}\)간의 cross-entropy<sup class="footnote-ref"><a href="#fn13" id="fnref13">[13]</a></sup>이다.</p>
<p>손실함수의 값을 최대한 작게 만들어야 품질이 좋아지게 되는데 전체 단어 집합인 |W|에 대해 모두 계산해야 한다.<br>
보통 이런 단어 집합은 수십만개나 되고 손실함수가 바뀔 때마다 모두 계산을 해야 하므로 매우 비효율적이다.</p>
<p>이 부분을 최소화하기 위해 &quot;2013b&quot;에서 Hirerarchical Softmax와 네거티브 샘플링(Negative Sampling)이 제시되었다.<br>
두 가지 방법을 각각 적용하여 성능을 평가한 결과 네거티브 샘플링의 성능이 좋았고 CROW보다 Skip-gram으로 적용했을 때 성능이 더 좋았다.<br>
따라서 이후에는 Skip-gram에 네거티브 샘플링을 적용한 방법이 일반적으로 많이 쓰이게 되었고 SGNS(Skip-Gram with Negative Sampling)이라는 말로 word2vec을 대체하기도 했다.<br>
2014년 발표된 Glove와 함께 SGNS는 단어레벨 임베딩의 양대 산맥으로써 자리잡았다.</p>
<h4 id="Hirerarchical-Softmax">Hirerarchical Softmax</h4>
<p>모든 단어가 나온 횟수를 센 다음 이진 트리를 구성하여 트리의 최초 단어(root)에는 말뭉치 상에서 가장 높은 빈도로 등장한 단어를, 그 자식 노드에는 2,3번째로 등장한 단어를, 각각의 다음 자식 노드에는 그 다음으로 많이 등장한 단어를 지정하는 방식으로 구성한다.<br>
이 트리는 softmax 함수의 산출값을 구할 때 루트에서부터 내가 확률값을 구하고자 하는 단어가 저장된 리프(leaf) 노드까지 가는 길에 저장된 확률을 곱해나가는 식으로 이를 계산할 수 있도록 도와줍니다.<br>
즉, 이 트리를 사용하면 임의의 어휘 벡터에 대한 softmax 산출값을 구하기 위해 모두 탐색하지 않고 적은 횟수로 softmax값을 구할 수 있다.</p>
<h4 id="Negative-Sampling">Negative Sampling</h4>
<p>어떤 말뭉치에서 등장한 어휘가 n개라면 특정 맥락에서 나와야 하는 단어 몇 개, 나오면 안되는 단어 몇 개만 추려서 계산해보자는 관점에서 출발하였다.<br>
네거티브 샘플링 방식으로 학습하게 되면 1개의 포지티브 샘플과 k개의 네거티브 샘플만 계산하면 되기 때문에 모델을 단계마다 전체 단어를 모두 계산하는 방식보다는 빠르게 진행된다.</p>
<p>예를 들어 “여름 아이스크림 벤치 시원 기분”(여름에 아이스크림을 벤치에서 먹으면 시원하고 기분이 좋다)에서 &quot;아이스크림&quot;을 중심단어로 보고 “여름”,&quot;벤치&quot;를 맞추는 모델을 학습한다고 하면 “여름”,&quot;벤치&quot;는 포지티브 샘플이 되고 수십만개의 네가티브 샘플이 생기게 된다.<br>
여기서 네가티브 샘플링할 수를 2개로 제한한다면 {“여름”-“자동차”,“학교”},{“벤치”-“구름”,“비”} 등과 같이 구성하여 간단한 이진 분류형태로 변경할 수 있다.</p>
<p>그런데 여기서 문제가 발생한다. 수십만개의 네가티브 샘플에서 어떤 단어를 뽑아서 쓸 것인가 정하기 어렵다.<br>
이런 부분을 해결하기 위해 Noise Distribution을 정의하고 그 분포를 이용해 단어들을 뽑아서 사용하는데 논문에서는 Unigram Distribution<sup class="footnote-ref"><a href="#fn14" id="fnref14">[14]</a></sup>의 3/4승을 이용해서 좋은 결과를 낸 것으로 되어 있다.<br>
\({U(w_i)}\)은 해당 단어의 Unigram확률로 (해당 단어 수 / 전체 단어 수)으로 나타낼 수 있다.</p>
<p>$$P_{negative}(w_i) = { {U(w_i)}^{3/4} \over {\sum_{j=0}^n U(w_j)}^{3/4} } $$</p>
<p><em>TODO:예제와 이진분류형태로 어떻게 변하는지 다시 구성</em></p>
<h3 id="FastText">FastText</h3>
<p>SGNS로 학습된 단어레벨의 임베딩으로 인해 눈에 띄게 발전하면서 많은 분야에 적용되었다.<br>
이 과정에서 word2vec의 단점이 나타나게 되는데 영어가 아닌 언어에 대해 적용하면 여러가지 문제가 생긴다는 것이었다.</p>
<p>한국어와 같은 교착어는 동일한 단어가 문맥 속에서 문법적 규칙이 복잡하게 적용된다.<br>
의미상 유사하지만 문법적으로 조금만 다르면 서로 전혀 상관없는 단어가 되는 경우가 생기고 이에 대해 어휘에 대해 임베딩을 구성할 방법이 없었다.</p>
<p>이러한 문제를 해결하기 위한 시도가 FastText이다.</p>
<p>2015년부터 텍스트에서 단어보다 낮은 단위의 입력(n-gram 등)을 입력하면 더 좋은 결과를 낸다는 논문들이 나오기 시작했는데 특히 글자 수준(character-level)으로 나누어 학습하면 더 좋은 결과를 낸다는 관점에서 단어레벨 임베딩을 적용한 것이 Facebook AI Research(FAIR)에서 개발한 FastText이다.</p>
<p>FastText는 SGNS를 기반으로 하지만 최소 단위가 단어가 아닌 n-gram 단위로 내렸다는 것이다.</p>
<p>구체적으로 단어간 구분을 &lt;,&gt;로 하고 tri-gram에 대한 벡터를 만들게 됩니다. 이러한 방법을 Subword-Information Skip-Gram(SISG)라고 한다.<br>
이러한 방식을 적용하면 n-gram 수준에서 학습하기 때문에 동일한 의미를 갖는 어휘가 문법적인 규칙에 따라서 변화하는 패턴을 학습하기 쉬워진다.<br>
이렇게 하게 되면 적은 양의 데이터를 가지고 더 많은 학습 데이터를 만들 수 있어서 성능이 올라가는 효과가 있다.</p>
<p>또, 학습할 때 존재하지 않았던 어휘(OOV, out of vocabulary)에 대한 임베딩까지 적용할 수 있는데 예를 들면 &quot;군고구마&quot;라는 단어를 학습하지 않았더라도 &quot;군&quot;과 &quot;고구마&quot;라는 n-gram을 학습한 적이 있다면 두 임베딩 벡터를 조합하여 임베딩 벡터를 만들어 낼 수 있기 때문에 생소한 단어에 대해서도 word2vec에 비해 월등한 성능을 보인다고 증명되었다.</p>
<p>한국어 임베딩의 경우에도 word2vec과 비교했을 때 FastText로 학습한 단어레벨 임베딩의 성능이 더 좋다는 것이 확인되었다.</p>
<h3 id="GloVe-Global-Word-Vectors">GloVe(Global Word Vectors)</h3>
<p>2014년 미국 스탠포드대학에서 개발한 임베딩 방법론으로 임베딩된 두 단어 벡터의 내적이 말뭉치 전체에서의 동시 등장확률 로그값이 되도록 목적함수를 정의하였다. 논문에서는 LSA는 말뭉치 전체의 통계적인 정보를 모두 활용하지만 LSA 결과물을 가지고 단어/문서 간 유사도를 측정하기 어렵고 Word2Vec은 사용자가 지정한 윈도우 내에서만 학습/분석이 이뤄지게 때문에 말뭉치 전체의 공기정보(co-occurrence)는 반영하기 어렵다고 밝혔다.</p>
<p>GloVe는 LSA의 카운트 기반의 방법과 Word2Vec의 예측 기반의 방법을 모두 사용하여 구현하였다.</p>
<p><a href="https://wikidocs.net/22885">https://wikidocs.net/22885</a><br>
<a href="https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/04/09/glove/">https://ratsgo.github.io/from frequency to semantics/2017/04/09/glove/</a></p>
<h4 id="윈도우-기반-동시-등장-행렬-Windows-based-co-occurrence-matrix">윈도우 기반 동시 등장 행렬(Windows based co-occurrence matrix)</h4>
<p>단어의 동시 등장 행렬은 행과 열을 전체 단어 집합의 단어들로 구성하고 \(i\) 단어의 윈도우<sup class="footnote-ref"><a href="#fn12" id="fnref12:1">[12:1]</a></sup> 크기 내에서 \(k\) 단어가 등장한 횟수를 \(i\)행 \(k\)열에 기재한 행렬을 만한다.<br>
예를 들어 다음과 같은 문장이 있다고 가정하고 윈도우 크기를 1이라고 한다면:</p>
<ol>
<li>나는 인공지능을 공부한다.</li>
<li>나는 자율주행 자동차가 좋다.</li>
<li>나는 운전이 좋다.</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">나는</th>
<th style="text-align:center">인공지능을</th>
<th style="text-align:center">공부한다</th>
<th style="text-align:center">자율주행</th>
<th style="text-align:center">자동차가</th>
<th style="text-align:center">좋다</th>
<th style="text-align:center">운전이</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">나는</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">인공지능을</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">공부한다</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">자율주행</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">자동차가</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">좋다</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">운전이</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
</tr>
</tbody>
</table>
<p><em>예제는 이해를 돕기 위한 부분이고 실제와 다를 수 있음</em></p>
<h4 id="동시-등장-확률-co-occurence-probability">동시 등장 확률(co-occurence probability)</h4>
<p>동시 등장 확률 \(P(k|i)\)는 동시 등장 행렬로부터 특정 단어 \(i\)의 전체 등장 횟수를 합하고 특정 단어 \(i\)가 등장했을 때 어떤 단어 \(k\)가 등장한 횟수를 합하여 계산한 조건부 확률이다.</p>
<p>\(P(k|i)\)에서 \(i\)를 중심단어, \(k\)를 주변단어라고 했을 때, 동시 등장 행렬에서 중심단어 \(i\)의 행의 모든 값을 더한 값을 분모로 하고 \(i\)행 \(k\)열의 값을 분자로 한 값이라고 할 수 있다.</p>
<p><em>TODO:확률계산 결과 필요</em></p>
<h4 id="손실함수">손실함수</h4>
<p>GloVe의 단어 벡터 학습방식은 co-occurrence가 있는 두 단어의 단어 벡터를 이용하여 co-occurrence 값을 예측하는 regression 문제를 풉니다.</p>
<p>GloVe는 &quot;임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만드는 것&quot;이다.</p>
<p>GloVe의 연구진들은 벡터 \( w_i \), \( w_k \), \( \tilde w_k \)를 가지고 어떤 함수 \( F \)를 수행하면 \( \frac {P_ik}{P_jk} \)가 나온다는 초기 식으로부터 시작하는 걸 제안했다.</p>
<p>$$ F(w_i, w_j, \tilde w_k) = \frac {P_ik}{P_jk} $$</p>
<p>함수 F는 두 단어의 사이의 동시 확률의 크기 관계 비율 정보를 벡터 공간에 인코딩하는 것이 목적이며 \( w_i \)와 \( w_j \)라는 두 벡터의 차이를 함수 \( F \)의 입력으로 사용한다.</p>
<p>$$ F(w_i - w_j, \tilde w_k) = \frac {P_ik}{P_jk} $$</p>
<p>위 수식대로 보면 우변은 스칼라값이고 좌변은 벡터값이 된다. 이를 맞춰주기 위해 \( F \)의 입력에 내적(dot product)를 수행한다.</p>
<p>$$ F(( w_i - w_j)^T, \tilde w_k) = \frac {P_ik}{P_jk} $$</p>
<p>여기서 함수 \(F\)가 만족해야 할 필수 조건이 있는데 중심 단어 \(w\)와 주변 단어 \( \tilde w \)는 무작위 선택이므로 이 둘의 관계는 자유롭게 바뀔 수 있도록 해야 한다. 이것을 성립되게 하기 위해서 함수 \( F \)가 실수의 덧셈과 양수의 곱셈에 대해서 준동형(Homomorphism)<sup class="footnote-ref"><a href="#fn15" id="fnref15">[15]</a></sup>을 만족하도록 해야 한다.<br>
이를 수식으로 나타내면 아래와 같다.<br>
$$ F(a+b) = F(a)F(b), \forall a,b \in \mathbb{R} $$</p>
<p>이제 GloVe식을 다시 보면 함수 \(F\)는 결과값으로 스칼라 값인 \(\frac{P_ik}{P_jk}\)가 나와야 한다. 이렇게 되려면 \(a\)와 \(b\)가 각각 두 벡터의 내적값이 되어야 한다.</p>
<p>$$F(v_1^T v_2 + v_3^T v_4) = F(v_1^T v_2)F(v_3^T v_4), \forall v_1,v_2,v_3,v_4 \in V, V:vector $$</p>
<p>그런데 앞서 작성한 식에서는 \(w_i\)와 \(w_j\)라는 두 벡터의 차이를 함수 \(F\)의 입력으로 받고 이를 준동형식으로 변경하면 아래와 같다.</p>
<p>$$F(v_1^T v_2 - v_3^T v_4) = \frac{F(v_1^T v_2)}{F(v_3^T v_4)}, \forall v_1,v_2,v_3,v_4 \in V, V:vector $$</p>
<p>이제 준동형을 GloVe 식에 적용해보면 아래와 같다.</p>
<p>$$F((w_i - w_j)^T \tilde w_k) = \frac{F(w_i^T \tilde w_k)}{F(w_j^T \tilde w_k)} $$</p>
<p>여기서 우변은 본래 \(\frac{P_ik}{P_jk}\) 였으므로 다음과 같이 표현될 수 있다.</p>
<p>$$ \frac{P_ik}{P_jk} = \frac{F(w_i^T \tilde w_k)}{F(w_j^T \tilde w_k)} $$</p>
<p>$$ F(w_i^T \tilde w_k) = P_ik = \frac {X_ik}{X_i} $$</p>
<p>이제 함수 \(F\)를 찾아야 하는데 이를 정확하게 일치하는 함수가 바로 지수함수이다. \(F\)를 지수함수 \(exp\)라고 해보자.</p>
<p>$$ exp(w_i^T \tilde w_k - w_j^T \tilde w_k) = \frac{exp(w_i^T \tilde{w_k})}{exp(w_j^T \tilde{w_k})} $$</p>
<p>$$ exp(w_i^T \tilde w_k) = P_ik = \frac{X_ik}{X_i} $$</p>
<p>위의 두번째 식을 로그함수로 변환하면 아래와 같다.</p>
<p>$$ w_i^T \tilde w_k = log P_ik = log(\frac{X_ik}{X_i}) = log(X_ik) - log(X_i) $$</p>
<p>단, 제한사항에서 언급했듯이 \(w_i\)와 \(\tilde w_k\)는 두 값의 위치를 바꿔도 식이 성립되어야 한다. 그런데 이게 성립되려면 \(log(X_i)\)항이 문제가 된다. 그래서 논문에서는 \(log(X_i)\)항을 \(w_i\)에 대한 편향 \(b_i\)라는 상수항으로 대체하기로 한다.</p>
<p>$$w_i^T \tilde w_k + b_i + \tilde b_k = log (X_ik) $$</p>
<p>우변의 값과의 차이를 최소화하는 방향으로 좌변의 4개의 항은 학습을 통해 값이 바뀌는 변수들이 된다. 즉 손실함수는 아래와 같이 정의할 수 있다.</p>
<p>$$ Loss function = \sum_{m,n=1}^V (w_m^T \tilde w_n + b_m + \tilde b_n - log X_{mn})^2 $$</p>
<p>여기서 \(V\)는 단어 집합의 크기를 의미한다.</p>
<p>여기까지 정의하고나서도 연구진은 \(log X_ik\)에서 \(X_ik\)값이 0가 될 수 있음을 지적했고 그 대안 중 하나로 \(log X_ik \)항을 \(log(q+X_ik)\)로 변경한다.</p>
<p>하지만 여기까지 와도 해결안되는 부분이 있는데 동시 등장 행렬 \(X\)는 마치 백오브워즈에서 처럼 희소 행렬일 가능성이 다분하다는 점이다.</p>
<h3 id="Swivel-Submatrix-Wise-Vector-Embedding-Learner">Swivel(Submatrix-Wise Vector Embedding Learner)</h3>
<p><a href="https://arxiv.org/pdf/1602.02215.pdf">https://arxiv.org/pdf/1602.02215.pdf</a></p>
<h2 id="문장레벨의-검색">문장레벨의 검색</h2>
<h3 id="Levenshtein-Distance-Edit-Distance">Levenshtein Distance(Edit Distance)</h3>
<p><a href="https://lovit.github.io/nlp/2018/08/28/levenshtein_hangle/">https://lovit.github.io/nlp/2018/08/28/levenshtein_hangle/</a></p>
<h3 id="Doc2Vec">Doc2Vec</h3>
<h3 id="ELMo">ELMo</h3>
<h3 id="BERT">BERT</h3>
<h2 id="참고자료">참고자료</h2>
<ol>
<li>이기창, 한국어 임베이딩, 에이콘, 2019.</li>
<li>임희석, 자연어처리 바이블, 휴먼싸이언스, 2019.</li>
<li>하가사나카 류이치로. 아무것도 모르고 시작하는 인공지능 첫걸음. 한빛미디어, 2018.</li>
<li><a href="https://cs224d.stanford.edu/lecture_notes/notes1.pdf">CS224D: Deep Learning for NLP</a>, Stanford, 2016.</li>
<li>원준, <a href="https://wikidocs.net/book/2155">딥러닝을 위한 자연어 처리 입문</a>, Wikidocs, 2020.</li>
<li>Noam Shazeer외 3인, <a href="https://arxiv.org/pdf/1602.02215.pdf">Swivel: Improving Embeddings by Noticing What’s Missing</a>, Google, 2016.</li>
</ol>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p><a href="https://ko.dict.naver.com/#/entry/koko/a2702afdf0824333afc60999678e1793&amp;directAnchor=s322187p206155d259766">자연언어</a>,표준국어대사전(네이버사전) <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p><a href="https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string-search_algorithm">Boyer–Moore string-search algorithm</a>, wikipedia <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p><a href="https://ko.dict.naver.com/#/entry/koko/33d35452864548519b37791c43056c94">표제어</a>,고려대한국어사전(네이버사전) <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p><a href="https://en.wikipedia.org/wiki/Binary_search_algorithm">Binary Search</a>, wikipedia <a href="#fnref4" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p><a href="https://en.wikipedia.org/wiki/Trie">Trie</a>, wikipedia <a href="#fnref5" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn6" class="footnote-item"><p><a href="https://ko.wikipedia.org/wiki/%ED%98%95%ED%83%9C_%EB%B6%84%EC%84%9D">형태분석</a>, wikipedia <a href="#fnref6" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn7" class="footnote-item"><p><a href="https://en.wikibooks.org/wiki/Data_Compression/The_zero-frequency_problem">Data Compression/The zero-frequency problem</a>, wikibook <a href="#fnref7" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn8" class="footnote-item"><p><a href="/Artificial%20Intelligence#%EC%9B%90%ED%95%AB%EC%9D%B8%EC%BD%94%EB%94%A9-one-hot-encoding">원핫벡터</a> <a href="#fnref8" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn9" class="footnote-item"><p><a href="https://arxiv.org/pdf/1301.3781.pdf">Efficient Estimation of Word Representations in Vector Space(Mikolov et al., 2013a)</a> <a href="#fnref9" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn10" class="footnote-item"><p><a href="https://arxiv.org/pdf/1310.4546.pdf">Distributed Representation of Words and Phrase and their Compositionality(Mikolov et al., 2013b)</a> <a href="#fnref10" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn11" class="footnote-item"><p><a href="">네거티브 샘플링</a> <a href="#fnref11" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn12" class="footnote-item"><p><a href="/Artificial%20Intelligence/#window">윈도우</a> <a href="#fnref12" class="footnote-backref">↩︎</a> <a href="#fnref12:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn13" class="footnote-item"><p><a href="/Artificial%20Intelligence/#cross-entropy">cross-entropy</a> <a href="#fnref13" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn14" class="footnote-item"><p>unigram은 1-gram과 동일하며 unigram-distribution은 전체 데이터를 단어의 unigram으로 생성한 확률분포를 나타낸다. <a href="#fnref14" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn15" class="footnote-item"><p>\(X\)의 임의의 원소 \(x\), \(y\)와 연산 \(*\) 및 그에 대응되는 \(Y\)의 연산 \( \circ \)에 대해, \(f(x*y) = f(x) \circ f(y) \)를 만족한다. \( log(a \times b) = log(a) + log(b) \) 가 대표적인 예이다. <a href="https://namu.wiki/w/%EC%A4%80%EB%8F%99%ED%98%95%20%EC%82%AC%EC%83%81">준동형사상</a> <a href="#fnref15" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="" src="https://www.gravatar.com/avatar/ce8b6bf07418a33af34ecc50383675a7?s=128" alt="Lawn Seol"></figure><p class="title is-size-4 is-block line-height-inherit">Lawn Seol</p><p class="is-size-6 is-block">Software/Data Engineer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Asia/Seoul</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">2</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Category</p><a href="/categories"><p class="title">1</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">5</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/lawnseol" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/lawnseol"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/rss2.xml"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile is-mobile" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">License(BY-NC-SA)</span></span><span class="level-right"><span class="level-item tag">creativecommons.org</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/technology/"><span class="level-start"><span class="level-item">technology</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><h3 class="menu-label">Recent</h3><article class="media"><div class="media-content size-small"><p><time dateTime="2020-05-12T12:59:13.613Z">2020-05-12</time></p><p class="title is-6"><a class="link-muted" href="/Natural%20Language%20Process/">Natural Langauge Process</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/technology/">technology</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-04-20T12:06:13.891Z">2020-04-20</time></p><p class="title is-6"><a class="link-muted" href="/Artificial%20Intelligence/">Artificial Intelligence</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/technology/">technology</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2020/05/"><span class="level-start"><span class="level-item">May 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/04/"><span class="level-start"><span class="level-item">April 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ai/"><span class="tag">ai</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/"><span class="tag">인공지능</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC/"><span class="tag">자연어처리</span><span class="tag is-grey-lightest">1</span></a></div></div></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe to Updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button is-primary" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="https://www.gravatar.com/avatar/ce8b6bf07418a33af34ecc50383675a7?s=128" alt="Knowledges for life" height="28"></a><p class="size-small"><span>&copy; 2020 Lawn Seol</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'https://lawnseol.github.io',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to Top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><script>window.addEventListener("load", function () {
            MathJax.Hub.Config({
                'HTML-CSS': {
                    matchFontHeight: false
                },
                SVG: {
                    matchFontHeight: false
                },
                CommonHTML: {
                    matchFontHeight: false
                },
                tex2jax: {
                    inlineMath: [
                        ['$','$'],
                        ['\\(','\\)']
                    ]
                }
            });
        });</script><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>